# !/usr/bin/env python

# Copyright 2025 The HuggingFace Inc. team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

"""
Real Robot Environment Module - Plug-and-Play Component for test_env

This module provides a hot-pluggable real robot environment that can be directly
imported and used in test_env. It includes:

1. RobotEnv: Core Gym environment for real robot control
2. RealRobotEnvWrapper: Adapter that makes RobotEnv compatible with test_env interface
3. Human intervention support via teleoperator devices (gamepad/keyboard)
4. Inverse kinematics processing for end-effector control
5. Image preprocessing and observation processing pipelines

Usage Example:
    from robot.controller.piper.robot_env import RealRobotEnvWrapper
    
    # Create real robot environment
    env = RealRobotEnvWrapper(cfg, rank=0, world_size=1)
    
    # Use like test_env
    obs, info = env.reset()
    obs, reward, term, trunc, info = env.step(action)
    
    # Check for human intervention
    if info["intervene_flag"].any():
        intervened_action = info["intervene_action"]
        
    env.close()

Key Components:
    - RobotEnv: Low-level robot control (reset, step, observation)
    - make_robot_env: Factory to create robot + teleoperator
    - make_processors: Create data processing pipelines
    - step_env_and_process_transition: Unified step function with processors
    - RealRobotEnvWrapper: Main entry point for test_env integration
"""

import logging
logging.getLogger("can.interfaces.socketcan").setLevel(logging.ERROR)
import time
import sys
import os
from dataclasses import dataclass
from typing import Any
import cv2
import gymnasium as gym
import numpy as np
import torch
import math
# å°† lerobot è·¯å¾„æ·»åŠ åˆ° sys.pathï¼ˆç”¨äº RobotEnvï¼‰
_current_dir = os.path.dirname(os.path.abspath(__file__))
_lerobot_path = _current_dir
if _lerobot_path not in sys.path:
    sys.path.insert(0, _lerobot_path)

# æ ¸å¿ƒé…ç½®å’Œå·¥å…·
from lerobot.envs.configs import HILSerlRobotEnvConfig
from lerobot.model.kinematics import RobotKinematics

# ä» gym_manipulator_bipiper å¯¼å…¥åŸºç¡€ç¯å¢ƒå’Œå‡½æ•°
try:
    from lerobot.rl.gym_manipulator_bipiper import (
        RobotEnv,
        make_robot_env as _make_robot_env_base,
        make_processors as _make_processors_base,
        step_env_and_process_transition as _step_env_and_process_transition_base,
        reset_follower_position,
        control_loop,
        replay_trajectory,
    )
except ImportError as e:
    # å°è¯•ç›´æ¥å¯¼å…¥æ¨¡å—ç„¶åè®¿é—®
    import lerobot.rl.gym_manipulator_bipiper as gym_manipulator_bipiper_module
    RobotEnv = gym_manipulator_bipiper_module.RobotEnv
    _make_robot_env_base = gym_manipulator_bipiper_module.make_robot_env
    _make_processors_base = gym_manipulator_bipiper_module.make_processors
    _step_env_and_process_transition_base = gym_manipulator_bipiper_module.step_env_and_process_transition
    reset_follower_position = gym_manipulator_bipiper_module.reset_follower_position
    control_loop = gym_manipulator_bipiper_module.control_loop
    replay_trajectory = gym_manipulator_bipiper_module.replay_trajectory
# å¯¼å…¥processor ç±»
from lerobot.processor import (
    AddBatchDimensionProcessorStep,
    DataProcessorPipeline,
    DeviceProcessorStep,
    EnvTransition,
    Numpy2TorchActionProcessorStep,
    Torch2NumpyActionProcessorStep,
    TransitionKey,
    VanillaObservationProcessorStep,
    create_transition,
)
from lerobot.processor.converters import identity_transition

# Teleoperator ç›¸å…³ï¼ˆRealRobotEnvWrapper éœ€è¦ï¼‰
from lerobot.teleoperators.teleoperator import Teleoperator
from lerobot.teleoperators.utils import TeleopEvents

# å¸¸é‡å®šä¹‰
from lerobot.utils.constants import OBS_IMAGES, OBS_STATE

import pdb

def rotmat_to_rpy_zyx(R: np.ndarray) -> tuple[float, float, float]:
    """
    å°†æ—‹è½¬çŸ©é˜µ R è½¬æˆæ¬§æ‹‰è§’ (rx, ry, rz)ï¼Œå•ä½ rad
    ä½¿ç”¨ Z-Y-X (yaw-pitch-roll) çº¦å®šï¼š
      - rz: yaw (ç»• Z)
      - ry: pitch (ç»• Y)
      - rx: roll (ç»• X)
    è¿”å›é¡ºåºæŒ‰ EndPoseCtrl ä¹ æƒ¯ï¼šrx, ry, rz
    """
    # é¿å…æ•°å€¼è¯¯å·®
    r20 = -R[2, 0]
    r20_clamped = max(min(r20, 1.0), -1.0)
    pitch = math.asin(r20_clamped)

    cos_pitch = math.cos(pitch)
    if abs(cos_pitch) < 1e-6:
        # é€€åŒ–æƒ…å†µï¼ˆæ¥è¿‘ Â±90Â°ï¼‰ï¼Œç®€å•å¤„ç†
        roll = 0.0
        yaw = math.atan2(-R[0, 1], R[1, 1])
    else:
        roll = math.atan2(R[2, 1], R[2, 2])
        yaw = math.atan2(R[1, 0], R[0, 0])

    return float(roll), float(pitch), float(yaw)

def images_encoding(imgs):
    """ç¼–ç å›¾åƒå¹¶è®°å½•è€—æ—¶"""
    encode_start = time.perf_counter()
    encode_data = []
    padded_data = []
    max_len = 0
    for i in range(len(imgs)):
        success, encoded_image = cv2.imencode(".jpg", imgs[i])
        jpeg_data = encoded_image.tobytes()
        encode_data.append(jpeg_data)
        max_len = max(max_len, len(jpeg_data))
    for i in range(len(imgs)):
        padded_data.append(encode_data[i].ljust(max_len, b"\0"))
    encode_time = time.perf_counter() - encode_start
    return encode_data, max_len, encode_time
# å¼ºåˆ¶åˆ·æ–°è¾“å‡ºï¼Œè§£å†³ Ray ç¯å¢ƒä¸‹çš„è¾“å‡ºç¼“å†²é—®é¢˜
def force_print(*args, **kwargs):
    """å¼ºåˆ¶è¾“å‡ºå¹¶åˆ·æ–°çš„ print å‡½æ•°"""
    print(*args, **kwargs, flush=True)
    sys.stdout.flush()
    sys.stderr.flush()
# æ¨¡å—åŠ è½½æç¤º
force_print(f"[robot_env.py] Module loaded")


class HDF5DataLoader:
    """HDF5æ•°æ®åŠ è½½å™¨ï¼Œç”¨äºä»HDF5æ–‡ä»¶è¯»å–æœºå™¨äººæ•°æ®ã€‚
    
    æ¯ä¸ª .hdf5 æ–‡ä»¶ä»£è¡¨ä¸€ä¸ª episodeï¼Œæ–‡ä»¶ç»“æ„ä¸ºï¼š
    - action: (T, 14) åŠ¨ä½œåºåˆ—
    - observations/qpos: (T, 14) çŠ¶æ€åºåˆ—
    - observations/images/{cam_high, cam_left_wrist, cam_right_wrist}: (T, H, W, 3) å›¾åƒåºåˆ—
    """
    
    # ç›¸æœºåç§°æ˜ å°„ï¼šå†…éƒ¨åç§° -> HDF5æ–‡ä»¶ä¸­çš„åç§°
    CAMERA_NAME_MAP = {
        'front': 'cam_high',
        'left': 'cam_left_wrist', 
        'right': 'cam_right_wrist',
    }
    
    def __init__(
        self,
        data_path: str,
        episode_index: int | None = None,
        step_size: int = 1,
    ):
        """åˆå§‹åŒ–HDF5æ•°æ®åŠ è½½å™¨ã€‚
        
        Args:
            data_path: HDF5æ–‡ä»¶è·¯å¾„æˆ–ç›®å½•è·¯å¾„
            episode_index: æŒ‡å®šepisodeç´¢å¼•ï¼ˆæ–‡ä»¶ç´¢å¼•ï¼‰ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©
            step_size: æ•°æ®è¯»å–æ­¥é•¿
        """
        import h5py
        from pathlib import Path
        hdf5_data_path = "/home/agilex-home/agilex/lerobot_hil-serl/dataset/20251125T005_lsz001_01"
        # force_print(f"[HDF5DataLoader] __init__ CALLED! data_path={data_path}")
        
        self.data_path = Path(hdf5_data_path)
        self.step_size = step_size
        self.ptr = 0  # å¸§æŒ‡é’ˆï¼Œä»0å¼€å§‹
        self.h5_file = None
        self.episode_index = 0
        self.episode_length = 0
        
        force_print(f"[HDF5DataLoader] Checking path: {self.data_path}")
        force_print(f"[HDF5DataLoader] Path exists: {self.data_path.exists()}")
        force_print(f"[HDF5DataLoader] Is dir: {self.data_path.is_dir()}")
        
        # æŸ¥æ‰¾HDF5æ–‡ä»¶ï¼ˆæ¯ä¸ªæ–‡ä»¶å°±æ˜¯ä¸€ä¸ªepisodeï¼‰
        if self.data_path.is_file() and self.data_path.suffix == '.hdf5':
            self.hdf5_files = [self.data_path]
        elif self.data_path.is_dir():
            # æŒ‰æ–‡ä»¶åæ’åºï¼ˆepisode_0.hdf5, episode_1.hdf5, ...ï¼‰
            self.hdf5_files = sorted(
                list(self.data_path.glob('*.hdf5')),
                key=lambda x: int(x.stem.split('_')[-1]) if x.stem.split('_')[-1].isdigit() else 0
            )
            force_print(f"[HDF5DataLoader] Found {len(self.hdf5_files)} .hdf5 files")
            if not self.hdf5_files:
                raise ValueError(f"No HDF5 files found in {data_path}")
        else:
            raise ValueError(f"Invalid data_path: {data_path}")
        
        force_print(f"[HDF5DataLoader] Total episodes: {len(self.hdf5_files)}")
        
        # é€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªepisodeï¼ˆæ–‡ä»¶ï¼‰
        self.select_episode(episode_index)
    
    @property
    def num_episodes(self) -> int:
        """è¿”å›å¯ç”¨çš„episodeæ•°é‡"""
        return len(self.hdf5_files)
    
    def select_episode(self, episode_index: int | None = None):
        """é€‰æ‹©å¹¶åŠ è½½ä¸€ä¸ªepisodeï¼ˆHDF5æ–‡ä»¶ï¼‰ã€‚
        
        Args:
            episode_index: episodeç´¢å¼•ï¼ˆæ–‡ä»¶ç´¢å¼•ï¼‰ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©
        """
        import h5py
        
        # å…³é—­ä¹‹å‰çš„æ–‡ä»¶
        if self.h5_file is not None:
            self.h5_file.close()
            self.h5_file = None
        
        # é€‰æ‹©episodeç´¢å¼•
        num_episodes = len(self.hdf5_files)
        if episode_index is None:
            episode_index = np.random.randint(0, num_episodes)
        
        if episode_index < 0 or episode_index >= num_episodes:
            raise ValueError(f"Episode index {episode_index} out of range [0, {num_episodes})")
        
        self.episode_index = episode_index
        
        # åŠ è½½HDF5æ–‡ä»¶
        file_path = self.hdf5_files[episode_index]
        self.h5_file = h5py.File(file_path, 'r')
        
        # è·å–episodeé•¿åº¦ï¼ˆä»actionæˆ–qposçš„ç¬¬ä¸€ç»´ï¼‰
        if 'action' in self.h5_file:
            self.episode_length = self.h5_file['action'].shape[0]
        elif 'observations' in self.h5_file and 'qpos' in self.h5_file['observations']:
            self.episode_length = self.h5_file['observations']['qpos'].shape[0]
        else:
            raise ValueError(f"Cannot determine episode length from {file_path}")
        
        # é‡ç½®å¸§æŒ‡é’ˆåˆ°0
        self.ptr = 0
        
        force_print(f"[HDF5DataLoader] Selected episode {episode_index}: {file_path.name} (length: {self.episode_length})")
    
    def get_data(self, advance: int = 0) -> dict | None:
        """è·å–å½“å‰å¸§æ•°æ®å¹¶å¯é€‰æ¨è¿›æŒ‡é’ˆã€‚
        
        Args:
            advance: æ¨è¿›æ­¥æ•°ï¼Œ0è¡¨ç¤ºä¸æ¨è¿›
            
        Returns:
            åŒ…å« 'state', 'image', 'action' çš„å­—å…¸ï¼Œæˆ–Noneï¼ˆepisodeç»“æŸï¼‰
        """ 
        if self.ptr >= self.episode_length:
            return None
        
        current_idx = self.ptr
        
        # è¯»å–çŠ¶æ€æ•°æ® (observations/qpos)
        state = np.array(self.h5_file['observations']['qpos'][current_idx], dtype=np.float32)
        
        # è¯»å–å›¾åƒæ•°æ® (observations/images/{cam_high, cam_left_wrist, cam_right_wrist})
        image_data = {}
        images_group = self.h5_file['observations']['images']
        for internal_name, hdf5_name in self.CAMERA_NAME_MAP.items():
            if hdf5_name in images_group:
                img = np.array(images_group[hdf5_name][current_idx])
                # ç¡®ä¿å›¾åƒæ ¼å¼ä¸º (H, W, 3) ä¸”å€¼åœ¨ [0, 255]
                if img.ndim == 3 and img.shape[0] == 3:
                    img = np.transpose(img, (1, 2, 0))
                if img.dtype != np.uint8:
                    if img.max() <= 1.0:
                        img = (img * 255).astype(np.uint8)
                    else:
                        img = img.astype(np.uint8)
                image_data[internal_name] = img
        
        # è¯»å–åŠ¨ä½œæ•°æ®
        action = None
        if 'action' in self.h5_file:
            action = np.array(self.h5_file['action'][current_idx], dtype=np.float32)
        
        # æ¨è¿›æŒ‡é’ˆ
        if advance > 0:
            self.ptr += advance
        
        return {
            'state': {
                'state': state,
                'frame_index': current_idx,
                'episode_index': self.episode_index,
            },
            'image': image_data,
            'action': action,
        }
    
    def reset(self, episode_index: int | None = None):
        """é‡ç½®åˆ°æŒ‡å®šepisodeçš„å¼€å§‹ï¼ˆå¸§æŒ‡é’ˆå½’0ï¼‰ã€‚
        
        Args:
            episode_index: æ–°çš„episodeç´¢å¼•ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©æ–°episode
        """
        if episode_index is None or episode_index != self.episode_index:
            # åˆ‡æ¢åˆ°æ–°episodeï¼ˆNoneè¡¨ç¤ºéšæœºé€‰æ‹©ï¼Œæˆ–è€…æŒ‡å®šäº†ä¸åŒçš„episodeï¼‰
            self.select_episode(episode_index)
        else:
            # åªé‡ç½®å¸§æŒ‡é’ˆåˆ°0ï¼ˆç›¸åŒepisodeï¼Œä»å¤´å¼€å§‹ï¼‰
            self.ptr = 0
    
    def is_done(self) -> bool:
        """æ£€æŸ¥å½“å‰episodeæ˜¯å¦ç»“æŸã€‚"""
        return self.ptr >= self.episode_length
    
    @property
    def num_episodes(self) -> int:
        """è¿”å›episodeæ€»æ•°ï¼ˆHDF5æ–‡ä»¶æ•°é‡ï¼‰ã€‚"""
        return len(self.hdf5_files)
    
    def close(self):
        """å…³é—­HDF5æ–‡ä»¶ã€‚"""
        if self.h5_file is not None:
            self.h5_file.close()
            self.h5_file = None


class TestRobotEnv(gym.Env):
    """æµ‹è¯•ç”¨çš„æœºå™¨äººç¯å¢ƒï¼Œä»HDF5æ•°æ®é›†è¯»å–æ•°æ®ï¼Œä¸è¿æ¥çœŸå®ç¡¬ä»¶ã€‚
    
    æ¨¡æ‹ŸçœŸå®RobotEnvçš„æ¥å£ï¼Œä½¿ç”¨HDF5DataLoaderä»æ•°æ®é›†è¯»å–è§‚æµ‹å’ŒåŠ¨ä½œã€‚
    ç”¨äºæµ‹è¯•å’Œå¼€å‘ï¼Œä¸éœ€è¦çœŸå®æœºå™¨äººç¡¬ä»¶ã€‚
    """
    
    def __init__(
        self,
        use_gripper: bool = False,
        image_size: tuple[int, int] = (480, 640),
        num_cameras: int = 3,
        data_path: str = None,
        step_size: int = 1,
        action_chunk: int = 50,
    ) -> None:
        """åˆå§‹åŒ–æµ‹è¯•æœºå™¨äººç¯å¢ƒã€‚
        
        Args:
            use_gripper: æ˜¯å¦ä½¿ç”¨å¤¹çˆªï¼ˆæš‚æœªä½¿ç”¨ï¼‰
            image_size: å›¾åƒå°ºå¯¸ (height, width)
            num_cameras: ç›¸æœºæ•°é‡
            data_path: HDF5æ•°æ®é›†è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰
            step_size: æ•°æ®å›æ”¾æ­¥é•¿
            action_chunk: åŠ¨ä½œå—å¤§å°ï¼ˆæš‚æœªä½¿ç”¨ï¼‰
        """
        super().__init__()
        
        self.use_gripper = use_gripper
        self.image_size = image_size
        self.num_cameras = num_cameras
        # ä½¿ç”¨ä¼ å…¥çš„ data_path å‚æ•°
        self.data_path = data_path
        self.step_size = step_size
        
        force_print(f"[TestRobotEnv] __init__ CALLED! data_path={data_path}, self.data_path={self.data_path}")
        
        # ç›¸æœºåç§°
        self._image_keys = ["front", "left", "right"][:num_cameras]
        
        # æœ«ç«¯ä½å§¿ç»´åº¦ï¼šå·¦è‡‚7ç»´ + å³è‡‚7ç»´ = 14ç»´
        self.state_dim = 14
        
        # Episode tracking
        self.current_step = 0
        
        # ä½¿ç”¨ä¼ å…¥çš„ data_path å‚æ•°,å¦‚æœæ²¡æœ‰åˆ™ä½¿ç”¨ç¡¬ç¼–ç è·¯å¾„
        if self.data_path is None:
            hdf5_data_path = "/home/agilex-home/agilex/lerobot_hil-serl/dataset/20251125T005_lsz001_01"
            force_print(f"[TestRobotEnv] No data_path provided, using hardcoded path: {hdf5_data_path}")
        else:
            hdf5_data_path = self.data_path
            force_print(f"[TestRobotEnv] Using provided data_path: {hdf5_data_path}")
        
        # åˆå§‹åŒ– HDF5DataLoader
        self.use_hdf5 = True  # æ€»æ˜¯å°è¯•ä½¿ç”¨ HDF5
        force_print(f"[TestRobotEnv] Attempting to load HDF5 data from: {hdf5_data_path}")
        force_print(f"[TestRobotEnv] step_size={step_size}")
        try:
            force_print(f"[TestRobotEnv] Creating HDF5DataLoader...")
            self.hdf5_loader = HDF5DataLoader(
                data_path=hdf5_data_path,
                episode_index=None,  # éšæœºé€‰æ‹©episode
                step_size=step_size,
            )
            force_print(f"[TestRobotEnv] âœ“ HDF5DataLoader initialized successfully!")
            force_print(f"[TestRobotEnv] Number of episodes: {self.hdf5_loader.num_episodes}")
        except Exception as e:
            import traceback
            force_print(f"[TestRobotEnv] âœ— Failed to load HDF5: {e}")
            force_print(f"[TestRobotEnv] Traceback: {traceback.format_exc()}")
            force_print("[TestRobotEnv] Falling back to random data mode")
            self.use_hdf5 = False
            self.hdf5_loader = None
        
        self._setup_spaces()
    
    def _setup_spaces(self) -> None:
        """é…ç½®è§‚æµ‹å’ŒåŠ¨ä½œç©ºé—´ã€‚"""
        # å›¾åƒè§‚æµ‹ç©ºé—´
        observation_spaces = {}
        prefix = OBS_IMAGES
        for key in self._image_keys:
            observation_spaces[f"{prefix}.{key}"] = gym.spaces.Box(
                low=0, high=255, 
                shape=(self.image_size[0], self.image_size[1], 3), 
                dtype=np.uint8
            )
        
        # çŠ¶æ€è§‚æµ‹ç©ºé—´ï¼ˆæœ«ç«¯ä½å§¿ï¼‰
        observation_spaces[OBS_STATE] = gym.spaces.Box(
            low=-10.0,
            high=10.0,
            shape=(self.state_dim,),
            dtype=np.float32,
        )
        
        self.observation_space = gym.spaces.Dict(observation_spaces)
        
        # åŠ¨ä½œç©ºé—´ï¼š14ç»´ï¼ˆå·¦è‡‚7ç»´ + å³è‡‚7ç»´ï¼‰
        action_dim = 14
        self.action_space = gym.spaces.Box(
            low=-1.0,
            high=1.0,
            shape=(action_dim,),
            dtype=np.float32,
        )
    
    def _get_observation(self) -> dict[str, Any]:
        """è·å–è§‚æµ‹æ•°æ®ï¼ˆä»HDF5æˆ–éšæœºç”Ÿæˆï¼‰ã€‚"""
        if self.use_hdf5 and self.hdf5_loader is not None:
            # ä»æ•°æ®é›†è¯»å–ï¼ˆæ•°æ®å›æ”¾æ¨¡å¼ï¼‰
            # ä»HDF5æ•°æ®é›†è¯»å–
            data = self.hdf5_loader.get_data()
            if data is None:
                # Episodeç»“æŸï¼Œé‡ç½®åˆ°æ–°episode
                self.hdf5_loader.reset(episode_index=None)
                data = self.hdf5_loader.get_data()
            
            # ç›´æ¥ä½¿ç”¨HDF5DataLoaderè¿”å›çš„æ•°æ®
            images = data['image']
            state_data = data['state']
            
            # ä»state_dataä¸­æå–stateæ•°ç»„
            if isinstance(state_data, dict) and 'state' in state_data:
                state = state_data['state']
            else:
                state = state_data
            
            # è½¬æ¢stateä¸ºend_poseæ ¼å¼
            if isinstance(state, dict):
                agent_end_pose_value = np.array(list(state.values()), dtype=np.float32)
            else:
                agent_end_pose_value = np.asarray(state, dtype=np.float32)
            
            # ç¡®ä¿ç»´åº¦æ­£ç¡®
            if len(agent_end_pose_value) != self.state_dim:
                # å¦‚æœç»´åº¦ä¸åŒ¹é…ï¼Œå¡«å……æˆ–æˆªæ–­
                if len(agent_end_pose_value) < self.state_dim:
                    agent_end_pose_value = np.pad(
                        agent_end_pose_value, 
                        (0, self.state_dim - len(agent_end_pose_value)),
                        mode='constant'
                    )
                else:
                    agent_end_pose_value = agent_end_pose_value[:self.state_dim]
            # è½¬æ¢å›¾åƒæ ¼å¼ï¼šimagesæ˜¯å­—å…¸ {cam_name: img_array}
            pixels = {}
            for i, key in enumerate(self._image_keys):
                if key in images:
                    img = images[key]
                    # è°ƒæ•´å›¾åƒå°ºå¯¸ï¼ˆå¦‚æœéœ€è¦ï¼‰
                    if img.shape[:2] != self.image_size:
                        import cv2
                        img = cv2.resize(img, (self.image_size[1], self.image_size[0]))
                    pixels[key] = img
                else:
                    # å¦‚æœç¼ºå°‘æŸä¸ªç›¸æœºï¼Œä½¿ç”¨é»˜è®¤å›¾åƒ
                    pixels[key] = np.zeros(
                        (self.image_size[0], self.image_size[1], 3),
                        dtype=np.uint8
                    )
            
        else:
            # éšæœºç”Ÿæˆæ¨¡å¼ï¼ˆæ— æ•°æ®é›†ï¼‰
            agent_end_pose_value = np.random.uniform(-1.0, 1.0, size=(self.state_dim,)).astype(np.float32)
            
            pixels = {}
            for key in self._image_keys:
                pixels[key] = np.random.randint(
                    0, 256, 
                    size=(self.image_size[0], self.image_size[1], 3),
                    dtype=np.uint8
                )
        return {
            "agent_end_pose_value": agent_end_pose_value,
            "pixels": pixels
        }
    
    def reset(
        self, *, seed: int | None = None, options: dict[str, Any] | None = None
    ) -> tuple[dict[str, Any], dict[str, Any]]:
        """é‡ç½®ç¯å¢ƒã€‚
        
        Args:
            seed: éšæœºç§å­
            options: å…¶ä»–é€‰é¡¹
            
        Returns:
            (observation, info) å…ƒç»„
        """
        if seed is not None:
            np.random.seed(seed)
        
        self.current_step = 0
        
        # å¦‚æœä½¿ç”¨HDF5ï¼Œé‡ç½®åˆ°æ–°episode
        if self.use_hdf5 and self.hdf5_loader is not None:
            self.hdf5_loader.reset(episode_index=None)
        
        obs = self._get_observation()
        
        # æ„é€ è¿”å›æ ¼å¼
        result_obs = {}
        result_obs[OBS_STATE] = obs["agent_end_pose_value"]
        for key in self._image_keys:
            result_obs[f"{OBS_IMAGES}.{key}"] = obs["pixels"][key]
        
        info = {"is_success": False}
        return result_obs, info
    
    def step(self, action: np.ndarray, transition_info: dict[str, Any] | None = None) -> tuple[dict[str, Any], float, bool, bool, dict[str, Any]]:
        """æ‰§è¡Œä¸€æ­¥åŠ¨ä½œã€‚
        
        Args:
            action: åŠ¨ä½œæ•°ç»„ [14] (æœ«ç«¯ä½å§¿ç›®æ ‡)
            transition_info: è¿‡æ¸¡ä¿¡æ¯å­—å…¸ (ä¸ºäº†ä¸RobotEnvæ¥å£å…¼å®¹,TestRobotEnvä¸­ä¸ä½¿ç”¨)
            
        Returns:
            (observation, reward, terminated, truncated, info) å…ƒç»„
        
        Note:
            - æ•°æ®å›æ”¾æ¨¡å¼ï¼šaction è¢«è®°å½•ä½†ä¸å½±å“ç¯å¢ƒï¼ˆå›æ”¾å†å²æ•°æ®ï¼‰
            - éšæœºæ¨¡å¼ï¼šaction å¯ä»¥ç”¨äºç®€å•çš„çŠ¶æ€æ›´æ–°ï¼ˆå¯é€‰ï¼‰
            - transition_info å‚æ•°åœ¨ TestRobotEnv ä¸­è¢«å¿½ç•¥,ä»…ç”¨äºæ¥å£å…¼å®¹
        """
        # éªŒè¯ action æ ¼å¼
        if action is not None:
            action = np.asarray(action[:14], dtype=np.float32)
            if action.shape != (self.state_dim,):
                force_print(f"[TestRobotEnv] Warning: Action shape mismatch: expected ({self.state_dim},), got {action.shape}")
        
        self.current_step += 1
        
        # å¦‚æœä½¿ç”¨HDF5ï¼Œæ¨è¿›æ•°æ®æŒ‡é’ˆï¼ˆaction ä¸å½±å“å›æ”¾ï¼Œä½†ä¼šè¢«è®°å½•ï¼‰
        if self.use_hdf5 and self.hdf5_loader is not None:
            # æ¨è¿›æ•°æ®æŒ‡é’ˆ
            self.hdf5_loader.get_data(advance=self.step_size)
            
            # æ£€æŸ¥æ˜¯å¦åˆ°è¾¾episodeæœ«å°¾
            terminated = self.hdf5_loader.is_done()
            obs = self._get_observation()
        else:
            # éšæœºæ¨¡å¼ï¼šå¯ä»¥ä½¿ç”¨ action è¿›è¡Œç®€å•çš„çŠ¶æ€æ›´æ–°
            # è¿™é‡Œæˆ‘ä»¬ä¿æŒç®€å•ï¼Œåªç”Ÿæˆéšæœºè§‚æµ‹
            # å¦‚æœéœ€è¦ï¼Œå¯ä»¥ä½¿ç”¨ action æ¥æ›´æ–°çŠ¶æ€ï¼š
            #   next_state = current_state + action * 0.1  # ç®€å•çš„ç§¯åˆ†
            obs = self._get_observation()
            terminated = np.random.random() < 0.01  # 1%æ¦‚ç‡ç»ˆæ­¢
        
        # æ„é€ è¿”å›æ ¼å¼
        result_obs = {}
        result_obs[OBS_STATE] = obs["agent_end_pose_value"]
        for key in self._image_keys:
            result_obs[f"{OBS_IMAGES}.{key}"] = obs["pixels"][key]
        
        # å¥–åŠ±ï¼ˆç®€å•çš„ç¨€ç–å¥–åŠ±ï¼‰
        reward = 1.0 if terminated else 0.0
        
        # æ£€æŸ¥truncation
        truncated = self.current_step >= 1000  # æœ€å¤š1000æ­¥
        
        info = {
            "is_success": terminated,
            "current_step": self.current_step,
            "action_received": action is not None,  # è®°å½•æ˜¯å¦æ”¶åˆ° action
        }
        
        return result_obs, reward, terminated, truncated, info
    
    def get_end_pose_name(self) -> list[str]:
        """è¿”å›æœ«ç«¯ä½å§¿åç§°åˆ—è¡¨ã€‚"""
        left_names = [f"left_end_pose_{i}" for i in range(7)]
        right_names = [f"right_end_pose_{i}" for i in range(7)]
        return left_names + right_names
    
    def close(self):
        """å…³é—­ç¯å¢ƒã€‚"""
        if self.use_hdf5 and self.hdf5_loader is not None:
            self.hdf5_loader.close()

# RobotEnvã€reset_follower_position ç­‰å‡½æ•°å·²ä» gym_manipulator_bipiper å¯¼å…¥ï¼Œä¸åœ¨æ­¤é‡å¤å®ç°

def load_config_from_json(config_path: str) -> HILSerlRobotEnvConfig:
    """ä» JSON æ–‡ä»¶åŠ è½½é…ç½®å¹¶è½¬æ¢ä¸º HILSerlRobotEnvConfig å¯¹è±¡ã€‚
    
    Args:
        config_path: JSON é…ç½®æ–‡ä»¶è·¯å¾„
        
    Returns:
        HILSerlRobotEnvConfig å¯¹è±¡
    """
    import json
    import tempfile
    from pathlib import Path
    import draccus
    
    config_path = Path(config_path)
    if not config_path.exists():
        raise FileNotFoundError(f"Config file not found: {config_path}")
    
    force_print(f"[load_config_from_json] Loading config from: {config_path}")
    
    with open(config_path, 'r') as f:
        config_dict = json.load(f)
    
    # æå– env éƒ¨åˆ†çš„é…ç½®
    env_config = config_dict.get('env', {})
    
    force_print(f"[load_config_from_json] Config loaded: name={env_config.get('name', 'MISSING')}")
    
    # ä½¿ç”¨ draccus å°†å­—å…¸è½¬æ¢ä¸ºé…ç½®å¯¹è±¡
    # åˆ›å»ºä¸´æ—¶æ–‡ä»¶æ¥å­˜å‚¨ env é…ç½®
    with tempfile.NamedTemporaryFile('w', suffix='.json', delete=False) as f:
        json.dump(env_config, f)
        temp_config_file = f.name
    
    try:
        # ä½¿ç”¨ draccus è§£æé…ç½®
        with draccus.config_type("json"):
            cfg = draccus.parse(HILSerlRobotEnvConfig, temp_config_file, args=[])
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        Path(temp_config_file).unlink(missing_ok=True)
    
    return cfg


def make_robot_env(cfg: HILSerlRobotEnvConfig | str) -> tuple[gym.Env, Any]:
    """Create robot environment from configuration.
    
    æ‰©å±•ç‰ˆæœ¬ï¼Œæ”¯æŒ JSON é…ç½®åŠ è½½å’Œ gym_testenv æ¨¡å¼ã€‚
    å¯¹äºçœŸå®æœºå™¨äººç¯å¢ƒï¼Œå¤ç”¨ gym_manipulator_bipiper ä¸­çš„åŸºç¡€å®ç°ã€‚

    Args:
        cfg: Environment configuration object or path to JSON config file.

    Returns:
        Tuple of (gym environment, teleoperator device).
    """
    # å¦‚æœä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²è·¯å¾„ï¼Œåˆ™åŠ è½½é…ç½®æ–‡ä»¶
    if isinstance(cfg, str):
        cfg = load_config_from_json(cfg)
    
    force_print(f"[make_robot_env] CALLED! cfg.name={getattr(cfg, 'name', 'MISSING')}")
    force_print(f"[make_robot_env] cfg.data_path={getattr(cfg, 'data_path', 'MISSING')}")
    
    # Check if this is a test environment (no real hardware)
    if cfg.name == "gym_testenv":
        # gym_testenv doesn't need robot or teleop config (can be None or have type=None)
        if cfg.robot is not None and getattr(cfg.robot, 'type', None) is not None:
            raise ValueError("gym_testenv does not support robot configuration")
        if cfg.teleop is not None and getattr(cfg.teleop, 'type', None) is not None:
            raise ValueError("gym_testenv does not support teleop configuration")
        
        # Extract gripper settings with defaults
        use_gripper = cfg.processor.gripper.use_gripper if cfg.processor.gripper is not None else True
        
        # è·å–æ•°æ®è·¯å¾„ï¼ˆå¦‚æœæä¾›ï¼‰
        data_path = getattr(cfg, 'data_path', None)
        step_size = getattr(cfg, 'step_size', 1)
        action_chunk = getattr(cfg, 'action_chunk', 50)
        
        # åˆ›å»ºæµ‹è¯•ç¯å¢ƒï¼ˆä½¿ç”¨Replayæˆ–éšæœºæ•°æ®ï¼‰
        env = TestRobotEnv(
            use_gripper=use_gripper,
            image_size=(480, 640),  # é»˜è®¤å›¾åƒå°ºå¯¸
            num_cameras=3,  # é»˜è®¤3ä¸ªç›¸æœº
            data_path=data_path,
            step_size=step_size,
            action_chunk=action_chunk,
        )
        
        return env, None
    
    # å¯¹äºå…¶ä»–ç¯å¢ƒç±»å‹ï¼ˆgym_hil å’Œ real_robotï¼‰ï¼Œå¤ç”¨åŸºç¡€å®ç°
    # ä½†éœ€è¦å…ˆåˆ‡æ¢åˆ° URDF ç›®å½•ï¼ˆå¦‚æœä½¿ç”¨çœŸå®æœºå™¨äººï¼‰
    if cfg.name != "gym_hil":
        from pathlib import Path
        current_file = Path(__file__).resolve()
        urdf_dir = current_file.parent / "local_assets"
        original_cwd = os.getcwd()
        
        try:
            force_print(f"[make_robot_env] Switching to URDF directory: {urdf_dir}")
            os.chdir(str(urdf_dir))
            env, teleop_device = _make_robot_env_base(cfg)
        finally:
            os.chdir(original_cwd)
            force_print(f"[make_robot_env] Restored working directory: {original_cwd}")
        return env, teleop_device
    else:
        # gym_hil ä¸éœ€è¦åˆ‡æ¢ç›®å½•
        return _make_robot_env_base(cfg)


def make_processors(
    env: gym.Env, teleop_device: Teleoperator | None, cfg: HILSerlRobotEnvConfig, device: str = "cpu"
) -> tuple[
    DataProcessorPipeline[EnvTransition, EnvTransition], DataProcessorPipeline[EnvTransition, EnvTransition]
]:
    """Create environment and action processors.
    
    æ‰©å±•ç‰ˆæœ¬ï¼Œæ”¯æŒ gym_testenv æ¨¡å¼ã€‚
    å¯¹äºå…¶ä»–ç¯å¢ƒç±»å‹ï¼Œå¤ç”¨ gym_manipulator_bipiper ä¸­çš„åŸºç¡€å®ç°ã€‚

    Args:
        env: Robot environment instance.
        teleop_device: Teleoperator device for intervention.
        cfg: Processor configuration.
        device: Target device for computations.

    Returns:
        Tuple of (environment processor, action processor).
    """
    # å¤„ç† gym_testenvï¼ˆæµ‹è¯•ç¯å¢ƒï¼Œæ— çœŸå®ç¡¬ä»¶ï¼‰
    if cfg.name == "gym_testenv":
        # ç®€å•çš„å¤„ç†ç®¡é“ï¼Œä¸éœ€è¦ä»‹å…¥æ£€æµ‹
        action_pipeline_steps = [
            Torch2NumpyActionProcessorStep(),
        ]

        env_pipeline_steps = [
            Numpy2TorchActionProcessorStep(),
            VanillaObservationProcessorStep(),
            AddBatchDimensionProcessorStep(),
            DeviceProcessorStep(device=device),
        ]

        return DataProcessorPipeline(
            steps=env_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
        ), DataProcessorPipeline(
            steps=action_pipeline_steps, to_transition=identity_transition, to_output=identity_transition
        )

    # å¯¹äºå…¶ä»–ç¯å¢ƒç±»å‹ï¼Œå¤ç”¨åŸºç¡€å®ç°
    return _make_processors_base(env, teleop_device, cfg, device)


def step_env_and_process_transition(
    env: gym.Env,
    transition: EnvTransition,
    action: torch.Tensor,
    env_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
    action_processor: DataProcessorPipeline[EnvTransition, EnvTransition],
    save_images: bool = False,
) -> EnvTransition:
    """
    Execute one step with processor pipeline.
    
    æ‰©å±•ç‰ˆæœ¬ï¼Œå¤ç”¨ gym_manipulator_bipiper ä¸­çš„åŸºç¡€å®ç°ã€‚
    save_images å‚æ•°å½“å‰æœªä½¿ç”¨ï¼ˆåŸºç¡€å®ç°ä¸­å·²åŒ…å«å›¾åƒä¿å­˜é€»è¾‘ï¼‰ã€‚

    Args:
        env: The robot environment
        transition: Current transition state
        action: Action to execute
        env_processor: Environment processor
        action_processor: Action processor
        save_images: Whether to save processed images to disk (default False, å·²å¼ƒç”¨)

    Returns:
        Processed transition with updated state.
    """
    # å¤ç”¨åŸºç¡€å®ç°ï¼ˆåŸºç¡€å®ç°ä¸­å·²åŒ…å«å›¾åƒä¿å­˜é€»è¾‘ï¼‰
    return _step_env_and_process_transition_base(
        env=env,
        transition=transition,
        action=action,
        env_processor=env_processor,
        action_processor=action_processor,
    )


class RealRobotEnvWrapper:
    """
    ç®€åŒ–çš„æœºå™¨äººç¯å¢ƒé€‚é…å™¨ï¼Œç”¨äºtest_envé›†æˆå’Œæ•°æ®å›çŒæµ‹è¯•ã€‚
    
    èŒè´£ï¼š
    1. åˆå§‹åŒ–æœºå™¨äººå’Œå¤„ç†å™¨
    2. reset() - é‡ç½®ç¯å¢ƒå¹¶è¿”å›è§‚æµ‹
    3. step(action) - æ‰§è¡ŒåŠ¨ä½œå¹¶è¿”å›obs, reward, terminated, truncated, info
    4. è§‚æµ‹æ ¼å¼è½¬æ¢ï¼ˆRobotEnv -> test_envï¼‰
    
    æ³¨æ„ï¼šchunk_stepç”±ä¸Šå±‚test_envå®ç°ï¼Œæ­¤å¤„ä¸å®ç°ã€‚
    
    Args:
        cfg: é…ç½®å¯¹è±¡
        rank: è¿›ç¨‹rank
        world_size: æ€»è¿›ç¨‹æ•°
    """
    
    def __init__(self, cfg, rank: int = 0, world_size: int = 1):
        force_print(f"[RealRobotEnvWrapper] __init__ CALLED! cfg type={type(cfg)}")
        force_print(f"[RealRobotEnvWrapper] cfg.env={getattr(cfg, 'env', 'MISSING')}")
        force_print(f"[RealRobotEnvWrapper] cfg.env.name={getattr(getattr(cfg, 'env', None), 'name', 'MISSING')}")
        
        self.cfg = cfg
        self.rank = rank
        self.world_size = world_size
        self.device = getattr(cfg, 'device', 'cpu')
        self.num_envs = getattr(cfg, 'num_envs', 1)
        
        # åˆå§‹åŒ–è¿åŠ¨å­¦æ±‚è§£å™¨(ç¼“å­˜,é¿å…æ¯æ¬¡stepéƒ½åˆ›å»º)
        self.left_kin = None
        self.right_kin = None
        self._init_kinematics()
        
        # åˆ›å»ºæœºå™¨äººç¯å¢ƒå’Œé¥æ“ä½œè®¾å¤‡
        # å¦‚æœæä¾›äº† robot_config_path,åˆ™ä»JSONæ–‡ä»¶åŠ è½½å®Œæ•´é…ç½®
        robot_config_path = getattr(cfg, 'robot_config_path', None)
        if robot_config_path is not None:
            force_print(f"[RealRobotEnvWrapper] Loading robot config from: {robot_config_path}")
            self.env, self.teleop_device = make_robot_env(robot_config_path)
            # åŒæ ·ä»JSONæ–‡ä»¶åŠ è½½å¤„ç†å™¨é…ç½®
            env_cfg = load_config_from_json(robot_config_path)
        else:
            force_print(f"[RealRobotEnvWrapper] Using Hydra config: cfg.env")
            self.env, self.teleop_device = make_robot_env(cfg.env)
            env_cfg = cfg.env
        
        # åˆ›å»ºå¤„ç†å™¨ç®¡é“
        self.env_processor, self.action_processor = make_processors(
            self.env, self.teleop_device, env_cfg, self.device
        )
        
        # å½“å‰transitionçŠ¶æ€
        self.current_transition = None
        
        # ç›¸æœºåç§°æ˜ å°„
        self.camera_mapping = getattr(cfg, 'camera_mapping', {
            'front': 'head_image',
            'left': 'left_wrist_image',
            'right': 'right_wrist_image',
        })
        
        # ä»»åŠ¡æè¿°
        self.task_description = getattr(cfg, 'task_description', 'catch_bowl')
        
        force_print(f"[RealRobotEnvWrapper] Initialized: rank={rank}, num_envs={self.num_envs}")
    
    def _init_kinematics(self):
        """åˆå§‹åŒ–è¿åŠ¨å­¦æ±‚è§£å™¨(åªåˆå§‹åŒ–ä¸€æ¬¡,é¿å…é¢‘ç¹åˆ›å»ºå¯¹è±¡)"""
        try:
            from pathlib import Path
            
            # ä½¿ç”¨ç›¸å¯¹è·¯å¾„ï¼šç›¸å¯¹äºå½“å‰æ–‡ä»¶ (robot_env.py) çš„ä½ç½®
            # robot_env.py ä½äº .../robot/controller/piper/robot_env.py
            # local_assets ä½äº .../robot/controller/piper/local_assets/
            current_file = Path(__file__).resolve()
            piper_dir = current_file.parent  # .../robot/controller/piper/
            
            # ğŸ”§ å…³é”®ä¿®å¤ï¼šä½¿ç”¨ local_assets/ ç›®å½•ä¸‹çš„ robot.urdf
            # è¿™æ · placo ä¼šåœ¨ local_assets/ ä¸‹æŸ¥æ‰¾ robot.urdfï¼Œ
            # å¹¶ä¸” mesh æ–‡ä»¶çš„ç›¸å¯¹è·¯å¾„ä¹Ÿæ˜¯ç›¸å¯¹äº local_assets/ çš„
            # mesh_dir = piper_dir / "local_assets"

            # urdf_path = mesh_dir  # ä¼ å…¥ç›®å½•,è®© placo è‡ªåŠ¨æŸ¥æ‰¾ robot.urdf
            urdf_path = "/home/agilex-home/agilex/keweijie/verl/recipe/vla/envs/robot_env/robot/controller/piper/local_assets/robot.urdf"
            
            # force_print(f"[RealRobotEnvWrapper] Current file: {current_file}")
            # force_print(f"[RealRobotEnvWrapper] Computed URDF path: {urdf_path}")
            # force_print(f"[RealRobotEnvWrapper] URDF exists: {urdf_path.exists()}")
            # force_print(f"[RealRobotEnvWrapper] Computed mesh dir: {mesh_dir}")
            # force_print(f"[RealRobotEnvWrapper] Mesh dir exists: {mesh_dir.exists()}")
            # force_print(f"[RealRobotEnvWrapper] Meshes subdir exists: {(mesh_dir / 'meshes').exists()}")
            
            # ğŸ”§ ä¸éœ€è¦åˆ‡æ¢å·¥ä½œç›®å½•ï¼Œç›´æ¥ä¼ å…¥ local_assets ç›®å½•
            # placo ä¼šåœ¨è¯¥ç›®å½•ä¸‹æŸ¥æ‰¾ robot.urdf å’Œ meshes/
            force_print(f"[RealRobotEnvWrapper] Initializing RobotKinematics...")
            self.left_kin = RobotKinematics(
                urdf_path=str(urdf_path),
                target_frame_name="joint6",
                joint_names=[f"joint{i+1}" for i in range(6)],
            )
            self.right_kin = RobotKinematics(
                urdf_path=str(urdf_path),
                target_frame_name="joint6",
                joint_names=[f"joint{i+1}" for i in range(6)],
            )
            force_print(f"[RealRobotEnvWrapper] RobotKinematics initialized successfully!")
        except Exception as e:
            import traceback
            force_print(f"[RealRobotEnvWrapper] WARNING: Failed to initialize kinematics: {e}")
            force_print(f"[RealRobotEnvWrapper] Traceback: {traceback.format_exc()}")
            force_print(f"[RealRobotEnvWrapper] Kinematics will be disabled")
            self.left_kin = None
            self.right_kin = None
    
    def _convert_obs_to_test_env_format(self, transition: EnvTransition) -> dict:
        """
        è½¬æ¢è§‚æµ‹æ ¼å¼ï¼šRobotEnv -> test_envã€‚
        
        RobotEnv format:
            observation.images.front: [1, C, H, W] tensor
            observation.images.left: [1, C, H, W] tensor
            observation.images.right: [1, C, H, W] tensor
            observation.state: [1, 14] tensor
        
        è¿”å›æ ¼å¼ï¼ˆå•ä¸ªç¯å¢ƒï¼Œéæ‰¹æ¬¡ï¼‰:
            {
                "images": {
                    "head_image": bytes (JPEG encoded),
                    "left_wrist_image": bytes (JPEG encoded),
                    "right_wrist_image": bytes (JPEG encoded)
                },
                "state": [14] float32 ndarray,
                "task_description": str
            }
        """
        obs = transition[TransitionKey.OBSERVATION]
        
        images_list = []
        camera_names = []
        
        # è½¬æ¢å›¾åƒ: [1, C, H, W] tensor -> [H, W, C] uint8 numpy
        for robot_cam_name, test_cam_name in self.camera_mapping.items():
            key = f"observation.images.{robot_cam_name}"
            if key in obs:
                img_tensor = obs[key]  # [1, C, H, W]
                if isinstance(img_tensor, torch.Tensor):
                    img_tensor = img_tensor.cpu()
                    # [1, C, H, W] -> [H, W, C]
                    img_np = img_tensor.squeeze(0).permute(1, 2, 0).numpy()
                    # [0, 1] float -> [0, 255] uint8
                    img_np = (img_np * 255).clip(0, 255).astype(np.uint8)
                    images_list.append(img_np)
                    camera_names.append(test_cam_name)
        
        # ç¡®ä¿æ‰€æœ‰æœŸæœ›çš„å›¾åƒéƒ½å­˜åœ¨
        default_image = np.zeros((224, 224, 3), dtype=np.uint8)
        for test_cam_name in ['head_image', 'left_wrist_image', 'right_wrist_image']:
            if test_cam_name not in camera_names:
                images_list.append(default_image)
                camera_names.append(test_cam_name)
        
        # ä½¿ç”¨ images_encoding å‡½æ•°å¯¹æ‰€æœ‰å›¾åƒè¿›è¡Œ JPEG ç¼–ç å‹ç¼©
        encoded_data, max_len, encode_time = images_encoding(images_list)
        
        # æ„å»ºå‹ç¼©åçš„å›¾åƒå­—å…¸
        # æ³¨æ„ï¼šéœ€è¦å°† bytes è½¬æ¢ä¸º torch.Tensor (uint8, 2D) æ ¼å¼
        # ä»¥ä¾¿ä¸ env_worker.py ä¸­çš„è§£ç é€»è¾‘å…¼å®¹
        images_dict = {}
        for i, cam_name in enumerate(camera_names):
            # å°† bytes è½¬æ¢ä¸º numpy arrayï¼Œç„¶åè½¬ä¸º tensor
            # ä½¿ç”¨ padded æ ¼å¼ç¡®ä¿æ‰€æœ‰å›¾åƒé•¿åº¦ä¸€è‡´
            padded_bytes = encoded_data[i].ljust(max_len, b"\0")
            # âš ï¸ å…³é”®ä¿®å¤: ä½¿ç”¨ np.array() è€Œä¸æ˜¯ np.frombuffer() åˆ›å»ºå¯å†™æ•°ç»„
            # np.frombuffer() è¿”å›åªè¯»æ•°ç»„,åœ¨Rayåˆ†å¸ƒå¼ä¼ è¾“æ—¶ä¼šå¤±è´¥
            img_array = np.array(np.frombuffer(padded_bytes, dtype=np.uint8))
            # è½¬æ¢ä¸º 2D tensor: [1, max_len] ä»¥ä¾¿åç»­ batch å¤„ç†
            images_dict[cam_name] = torch.from_numpy(img_array).unsqueeze(0)  # [1, max_len]
        
        # è½¬æ¢çŠ¶æ€
        state_array = np.zeros(14, dtype=np.float32)  # é»˜è®¤å€¼
        if "observation.state" in obs:
            state_tensor = obs["observation.state"]
            if isinstance(state_tensor, torch.Tensor):
                state_array = state_tensor.cpu().squeeze(0).numpy()
        
        return {
            "images": images_dict,
            "state": state_array,
            "task_description": self.task_description,
        }
    
    def reset(self, episode_id: int | None = None):
        """
        é‡ç½®ç¯å¢ƒåˆ°åˆå§‹çŠ¶æ€ã€‚
        
        Args:
            episode_id: æŒ‡å®šepisodeç´¢å¼•ï¼ˆä»…ç”¨äºgym_testenvï¼‰ï¼ŒNoneè¡¨ç¤ºéšæœºé€‰æ‹©
        
        Returns:
            obs_dict: test_envæ ¼å¼çš„è§‚æµ‹
                {
                    "images": {"head_image", "left_wrist_image", "right_wrist_image"},
                    "states": {"state"},
                    "task_description": str
                }
        """
        # å¦‚æœæ˜¯TestRobotEnvä¸”æä¾›äº†episode_idï¼Œéœ€è¦å…ˆè®¾ç½®episode
        if hasattr(self.env, 'hdf5_loader') and self.env.hdf5_loader is not None:
            if episode_id is not None:
                self.env.hdf5_loader.reset(episode_index=episode_id)
            else:
                self.env.hdf5_loader.reset(episode_index=None)
        
        # é‡ç½®æœºå™¨äººç¯å¢ƒ
        obs, info = self.env.reset()
        
        # é‡ç½®å¤„ç†å™¨
        self.env_processor.reset()
        self.action_processor.reset()
        
        # åˆ›å»ºå¹¶å¤„ç†åˆå§‹transition
        complementary_data = (
            {"raw_end_pose_value": info.pop("raw_end_pose_value")} 
            if "raw_end_pose_value" in info else {}
        )
        self.current_transition = create_transition(
            observation=obs, 
            info=info, 
            complementary_data=complementary_data
        )
        self.current_transition = self.env_processor(data=self.current_transition)
        
        # è½¬æ¢ä¸ºtest_envæ ¼å¼
        obs_converted = self._convert_obs_to_test_env_format(self.current_transition)
        
        return obs_converted
    
    def step(self, action):
        """
        æ‰§è¡Œä¸€æ­¥åŠ¨ä½œã€‚
        
        Args:
            action: åŠ¨ä½œ [action_dim] or [1, action_dim]
            
        Returns:
            obs_dict: è§‚æµ‹å­—å…¸
            reward: float å¥–åŠ±
            terminated: bool æ˜¯å¦æˆåŠŸç»ˆæ­¢
            truncated: bool æ˜¯å¦è¶…æ—¶
            info_dict: ä¿¡æ¯å­—å…¸ï¼ŒåŒ…å«intervene_actionå’Œintervene_flag
        """
        # è½¬æ¢ä¸ºtensor
        if isinstance(action, np.ndarray):
            action = torch.from_numpy(action).to(self.device)
        
        # ç¡®ä¿æ˜¯ [action_dim] æ ¼å¼
        if action.ndim == 2:
            action = action.squeeze(0)
        if action is not None:
            if isinstance(action, torch.Tensor):
                action_np = action.cpu().numpy()
            else:
                action_np = np.asarray(action, dtype=np.float32)
            
            # æ£€æŸ¥ç»´åº¦å¹¶å¡«å……
            original_dim = action_np.shape[0]
            if action_np.shape[0] == 7:
                # 7ç»´åŠ¨ä½œï¼Œå¡«å……å¦å¤–7ç»´ä¸º0ï¼ˆåŒè‡‚æœºå™¨äººï¼Œåªæ§åˆ¶ä¸€ä¸ªè‡‚ï¼‰
                action_np = np.concatenate([action_np, np.zeros(7, dtype=np.float32)])
                force_print(f"[RealRobotEnvWrapper] Action padded from 7 to 14 dims")
            elif action_np.shape[0] > 14:
                # è¶…è¿‡14ç»´ï¼Œæˆªæ–­
                action_np = action_np[:14]
                force_print(f"[RealRobotEnvWrapper] Action truncated from {original_dim} to 14 dims")
            elif action_np.shape[0] < 7:
                # å°äº7ç»´ï¼Œå¡«å……åˆ°14ç»´
                padding_size = 14 - action_np.shape[0]
                action_np = np.concatenate([action_np, np.zeros(padding_size, dtype=np.float32)])
                force_print(f"[RealRobotEnvWrapper] Action padded from {original_dim} to 14 dims")
            
            # æ£€æŸ¥æ˜¯å¦å·²åˆå§‹åŒ–è¿åŠ¨å­¦æ±‚è§£å™¨
            if self.left_kin is None or self.right_kin is None:
                force_print(f"[RealRobotEnvWrapper] WARNING: Kinematics not initialized, skipping FK conversion")
                # ç›´æ¥ä½¿ç”¨åŸå§‹action,ä¸åšè½¬æ¢
                action = torch.from_numpy(action_np).to(self.device)
            else:
                # æœºå™¨äººè¿åŠ¨å­¦ FK: joint -> end-effector pose
                # å…ˆè½¬å› tensor ä»¥ä¾¿ç´¢å¼•
                action_tensor = torch.from_numpy(action_np).to(self.device)
                
                left_joint_idx = [0, 1, 2, 3, 4, 5]
                left_grip_idx = 6
                right_joint_idx = [7, 8, 9, 10, 11, 12]
                right_grip_idx = 13
                
                # å–å‡ºå…³èŠ‚ï¼ˆradï¼‰-> numpy
                ql_rad = action_tensor[left_joint_idx].detach().cpu().numpy().astype(float)
                qr_rad = action_tensor[right_joint_idx].detach().cpu().numpy().astype(float)

                # RobotKinematics.forward_kinematics æ¥å£ä½¿ç”¨"åº¦"
                ql_deg = np.rad2deg(ql_rad)
                qr_deg = np.rad2deg(qr_rad)

                # åš FK (ä½¿ç”¨ç¼“å­˜çš„kinematicså¯¹è±¡)
                T_l = self.left_kin.forward_kinematics(ql_deg)   # 4x4
                T_r = self.right_kin.forward_kinematics(qr_deg)  # 4x4

                # ä½ç½®ï¼šç±³
                lx, ly, lz = T_l[:3, 3].tolist()
                rx, ry, rz = T_r[:3, 3].tolist()

                # å§¿æ€ï¼šæ—‹è½¬çŸ©é˜µ -> RPYï¼ˆå¼§åº¦ï¼‰
                lrx, lry, lrz = rotmat_to_rpy_zyx(T_l[:3, :3])
                rrx, rry, rrz = rotmat_to_rpy_zyx(T_r[:3, :3])

                # å¤¹çˆªï¼šä» action ä¸­è·å–
                l_grip = float(action_tensor[left_grip_idx].item())
                r_grip = float(action_tensor[right_grip_idx].item())

                # æ„å»º end-effector pose action (14ç»´)
                # [L_x, L_y, L_z, L_rx, L_ry, L_rz, L_grip,
                #  R_x, R_y, R_z, R_rx, R_ry, R_rz, R_grip]
                endpose_action = torch.tensor(
                    [
                        lx, ly, lz, lrx, lry, lrz, l_grip,
                        rx, ry, rz, rrx, rry, rrz, r_grip,
                    ],
                    device=self.device,
                    dtype=torch.float32,
                )
                
                # ä½¿ç”¨è½¬æ¢åçš„ end-effector pose action
                force_print(f"[RealRobotEnvWrapper] End-effector pose action: {endpose_action}")
                action = endpose_action
        
        # é€šè¿‡å¤„ç†å™¨ç®¡é“æ‰§è¡Œæ­¥éª¤ï¼ˆå¤„ç†äººå·¥ä»‹å…¥ï¼‰
        self.current_transition = step_env_and_process_transition(
            env=self.env,
            transition=self.current_transition,
            action=action,
            env_processor=self.env_processor,
            action_processor=self.action_processor,
        )
        
        # æå–æ•°æ®
        obs_converted = self._convert_obs_to_test_env_format(self.current_transition)
        reward = float(self.current_transition[TransitionKey.REWARD])
        terminated = bool(self.current_transition[TransitionKey.DONE])
        truncated = bool(self.current_transition[TransitionKey.TRUNCATED])
        
        # æå–ä»‹å…¥ä¿¡æ¯
        intervene_action = self.current_transition[TransitionKey.COMPLEMENTARY_DATA].get(
            "teleop_action", None
        )
        intervene_flag = self.current_transition[TransitionKey.INFO].get(
            TeleopEvents.IS_INTERVENTION, False
        )
        
        # æ„å»ºinfoå­—å…¸
        info_dict = {
            "intervene_action": intervene_action,
            "intervene_flag": intervene_flag,
        }
        
        return obs_converted, reward, terminated, truncated, info_dict
    
    def close(self):
        """Close environment and release resources."""
        if self.env is not None:
            self.env.close()
        if self.teleop_device is not None and hasattr(self.teleop_device, 'disconnect'):
            self.teleop_device.disconnect()

if __name__ == "__main__":
    """æµ‹è¯• RealRobotEnvWrapper èƒ½å¦æ­£ç¡®å¯åŠ¨ RobotEnv å’Œ TestRobotEnv"""
    import yaml
    from pathlib import Path
    from types import SimpleNamespace
    
    print("=" * 80)
    print("æµ‹è¯• RealRobotEnvWrapper å¯åŠ¨ RobotEnv å’Œ TestRobotEnv")
    print("=" * 80)
    
    def dict_to_obj(d):
        """é€’å½’åœ°å°†å­—å…¸è½¬æ¢ä¸ºå¯¹è±¡ï¼Œæ–¹ä¾¿ä½¿ç”¨ç‚¹å·è®¿é—®"""
        if isinstance(d, dict):
            return SimpleNamespace(**{k: dict_to_obj(v) for k, v in d.items()})
        elif isinstance(d, list):
            return [dict_to_obj(item) for item in d]
        else:
            return d
    
    # ====================================================================
    # æµ‹è¯• 1: ä½¿ç”¨ RealRobotEnvWrapper å¯åŠ¨ TestRobotEnvï¼ˆæ•°æ®å›æ”¾æ¨¡å¼ï¼‰
    # ====================================================================
    print("\n" + "=" * 80)
    print("æµ‹è¯• 1: RealRobotEnvWrapper + TestRobotEnvï¼ˆæ•°æ®å›æ”¾æ¨¡å¼ï¼‰")
    print("=" * 80)
    
    # åˆ›å»º gym_testenv é…ç½®
    testenv_cfg = SimpleNamespace(
        env=SimpleNamespace(
            name="gym_testenv",
            robot=None,
            teleop=None,
            processor=SimpleNamespace(
                gripper=SimpleNamespace(use_gripper=False, gripper_penalty=0.0),
                reset=SimpleNamespace(terminate_on_success=True, fixed_reset_joint_positions=None),
                observation=SimpleNamespace(display_cameras=False),
                inverse_kinematics=None
            ),
            data_path=None,  # None è¡¨ç¤ºä½¿ç”¨é»˜è®¤è·¯å¾„æˆ–ç”Ÿæˆéšæœºæ•°æ®
            step_size=1,
            action_chunk=50,
        ),
        num_envs=1,
        device="cpu",
        task_description="æµ‹è¯•ä»»åŠ¡",
        camera_mapping={
            'front': 'head_image',
            'left': 'left_wrist_image',
            'right': 'right_wrist_image',
        }
    )
    
    try:
        print("\n[1.1] åˆ›å»º RealRobotEnvWrapper (TestRobotEnv æ¨¡å¼)...")
        wrapper = RealRobotEnvWrapper(testenv_cfg, rank=0, world_size=1)
        print(f"  âœ“ å†…éƒ¨ç¯å¢ƒç±»å‹: {type(wrapper.env).__name__}")
        print(f"  âœ“ é¥æ“ä½œè®¾å¤‡: {wrapper.teleop_device}")
        
        print("\n[1.2] é‡ç½®ç¯å¢ƒ...")
        obs_dict = wrapper.reset()
        print(f"  âœ“ è§‚æµ‹é”®: {list(obs_dict.keys())}")
        print(f"  âœ“ å›¾åƒé”®: {list(obs_dict['images'].keys())}")
        print(f"  âœ“ çŠ¶æ€å½¢çŠ¶: {obs_dict['state'].shape}")
        print(f"  âœ“ ä»»åŠ¡æè¿°: {obs_dict['task_description']}")
        
        # æ˜¾ç¤ºå›¾åƒä¿¡æ¯ï¼ˆJPEG å‹ç¼©æ ¼å¼ï¼‰
        for cam_name, img_tensor in obs_dict['images'].items():
            print(f"    - {cam_name}: shape={img_tensor.shape}, dtype={img_tensor.dtype}")
        
        print("\n[1.3] æ‰§è¡ŒåŠ¨ä½œæµ‹è¯•...")
        num_steps = 3
        for step in range(num_steps):
            # ç”ŸæˆéšæœºåŠ¨ä½œ (14ç»´ end-effector pose)
            action = np.random.uniform(-1.0, 1.0, size=(14,)).astype(np.float32)
            
            # æ‰§è¡Œä¸€æ­¥
            obs_dict, reward, terminated, truncated, info_dict = wrapper.step(action)
            
            print(f"  æ­¥éª¤ {step+1}/{num_steps}: "
                  f"reward={reward:.4f}, "
                  f"terminated={terminated}, "
                  f"truncated={truncated}, "
                  f"intervene_flag={info_dict.get('intervene_flag', False)}")
            
            if terminated or truncated:
                print(f"    â†’ ç¯å¢ƒç»ˆæ­¢ï¼Œé‡ç½®...")
                obs_dict = wrapper.reset()
        
        print("\n[1.4] å…³é—­ç¯å¢ƒ...")
        wrapper.close()
        print("  âœ“ TestRobotEnv æ¨¡å¼æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        import traceback
        print(f"\n  âœ— TestRobotEnv æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
        print(f"  Traceback:\n{traceback.format_exc()}")
    
    # ====================================================================
    # æµ‹è¯• 2: ä½¿ç”¨ RealRobotEnvWrapper å¯åŠ¨ RobotEnvï¼ˆçœŸå®æœºå™¨äººæ¨¡å¼ï¼‰
    # ====================================================================
    print("\n" + "=" * 80)
    print("æµ‹è¯• 2: RealRobotEnvWrapper + RobotEnvï¼ˆçœŸå®æœºå™¨äººæ¨¡å¼ï¼‰")
    print("=" * 80)
    
    # ä½¿ç”¨ bipiper_gym_pico.json é…ç½®æ–‡ä»¶
    config_path = Path(__file__).parent / "config" / "bipiper_gym_pico.json"
    
    if not config_path.exists():
        print(f"  âš  é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {config_path}")
        print("  âš  è·³è¿‡çœŸå®æœºå™¨äººæµ‹è¯•")
    else:
        try:
            print(f"\n[2.1] åŠ è½½é…ç½®æ–‡ä»¶: {config_path}")
            
            # åˆ›å»ºé…ç½®å¯¹è±¡ï¼ˆä½¿ç”¨ robot_config_pathï¼‰
            robot_cfg = SimpleNamespace(
                robot_config_path=str(config_path),
                num_envs=1,
                device="cpu",
                task_description="çœŸå®æœºå™¨äººæµ‹è¯•ä»»åŠ¡",
                camera_mapping={
                    'front': 'head_image',
                    'left': 'left_wrist_image',
                    'right': 'right_wrist_image',
                }
            )
            
            print("\n[2.2] åˆ›å»º RealRobotEnvWrapper (RobotEnv æ¨¡å¼)...")
            print("  âš  æ­£åœ¨è¿æ¥çœŸå®æœºå™¨äººç¡¬ä»¶ï¼Œè¯·ç¨å€™...")
            wrapper = RealRobotEnvWrapper(robot_cfg, rank=0, world_size=1)
            print(f"  âœ“ å†…éƒ¨ç¯å¢ƒç±»å‹: {type(wrapper.env).__name__}")
            print(f"  âœ“ é¥æ“ä½œè®¾å¤‡ç±»å‹: {type(wrapper.teleop_device).__name__ if wrapper.teleop_device else None}")
            
            print("\n[2.3] é‡ç½®ç¯å¢ƒï¼ˆæœºå™¨äººå°†ç§»åŠ¨åˆ°åˆå§‹ä½ç½®ï¼‰...")
            print("  âš  æœºå™¨äººæ­£åœ¨ç§»åŠ¨ï¼Œè¯·ç¡®ä¿å®‰å…¨åŒºåŸŸæ— éšœç¢ç‰©...")
            obs_dict = wrapper.reset()
            print(f"  âœ“ é‡ç½®æˆåŠŸï¼")
            print(f"  âœ“ è§‚æµ‹é”®: {list(obs_dict.keys())}")
            print(f"  âœ“ å›¾åƒé”®: {list(obs_dict['images'].keys())}")
            print(f"  âœ“ çŠ¶æ€å½¢çŠ¶: {obs_dict['state'].shape}")
            print(f"  âœ“ ä»»åŠ¡æè¿°: {obs_dict['task_description']}")
            
            # æ˜¾ç¤ºå›¾åƒä¿¡æ¯
            for cam_name, img_tensor in obs_dict['images'].items():
                print(f"    - {cam_name}: shape={img_tensor.shape}, dtype={img_tensor.dtype}")
            
            print("\n[2.4] æ‰§è¡ŒåŠ¨ä½œæµ‹è¯•ï¼ˆå‘é€é›¶åŠ¨ä½œï¼Œæœºå™¨äººåº”ä¿æŒé™æ­¢ï¼‰...")
            num_steps = 5
            for step in range(num_steps):
                # å‘é€é›¶åŠ¨ä½œï¼ˆä¿æŒå½“å‰ä½ç½®ï¼‰
                action = np.zeros(14, dtype=np.float32)
                
                # æ‰§è¡Œä¸€æ­¥
                obs_dict, reward, terminated, truncated, info_dict = wrapper.step(action)
                
                print(f"  æ­¥éª¤ {step+1}/{num_steps}: "
                      f"reward={reward:.4f}, "
                      f"terminated={terminated}, "
                      f"truncated={truncated}, "
                      f"intervene_flag={info_dict.get('intervene_flag', False)}")
                
                if terminated or truncated:
                    print(f"    â†’ ç¯å¢ƒç»ˆæ­¢ï¼Œé‡ç½®...")
                    obs_dict = wrapper.reset()
                
                # çŸ­æš‚å»¶è¿Ÿ
                time.sleep(0.1)
            
            print("\n[2.5] å…³é—­ç¯å¢ƒ...")
            wrapper.close()
            print("  âœ“ RobotEnv æ¨¡å¼æµ‹è¯•é€šè¿‡ï¼")
            
        except KeyboardInterrupt:
            print("\n  âš  ç”¨æˆ·ä¸­æ–­æµ‹è¯•")
            if 'wrapper' in locals():
                try:
                    wrapper.close()
                except:
                    pass
        except Exception as e:
            import traceback
            print(f"\n  âœ— RobotEnv æ¨¡å¼æµ‹è¯•å¤±è´¥: {e}")
            print(f"  Traceback:\n{traceback.format_exc()}")
            if 'wrapper' in locals():
                try:
                    wrapper.close()
                except:
                    pass
    
    # ====================================================================
    # æµ‹è¯• 3: ç›´æ¥æµ‹è¯•åº•å±‚å‡½æ•°ï¼ˆmake_robot_env, make_processorsï¼‰
    # ====================================================================
    print("\n" + "=" * 80)
    print("æµ‹è¯• 3: åº•å±‚å‡½æ•°æµ‹è¯•ï¼ˆmake_robot_env + make_processorsï¼‰")
    print("=" * 80)
    
    try:
        print("\n[3.1] æµ‹è¯• make_robot_env (gym_testenv)...")
        cfg = SimpleNamespace(
            name="gym_testenv",
            robot=None,
            teleop=None,
            processor=SimpleNamespace(
                gripper=SimpleNamespace(use_gripper=False),
                reset=SimpleNamespace(terminate_on_success=True),
                observation=SimpleNamespace(display_cameras=False),
            ),
            data_path=None,
            step_size=1,
            action_chunk=50,
        )
        
        env, teleop_device = make_robot_env(cfg)
        print(f"  âœ“ ç¯å¢ƒç±»å‹: {type(env).__name__}")
        print(f"  âœ“ é¥æ“ä½œè®¾å¤‡: {teleop_device}")
        print(f"  âœ“ è§‚æµ‹ç©ºé—´: {env.observation_space}")
        print(f"  âœ“ åŠ¨ä½œç©ºé—´: {env.action_space}")
        
        print("\n[3.2] æµ‹è¯• make_processors...")
        env_processor, action_processor = make_processors(env, teleop_device, cfg, device="cpu")
        print(f"  âœ“ ç¯å¢ƒå¤„ç†å™¨æ­¥éª¤æ•°: {len(env_processor.steps)}")
        print(f"  âœ“ åŠ¨ä½œå¤„ç†å™¨æ­¥éª¤æ•°: {len(action_processor.steps)}")
        
        print("\n[3.3] æµ‹è¯•ç¯å¢ƒ reset + step...")
        obs, info = env.reset(seed=42)
        print(f"  âœ“ reset æˆåŠŸï¼Œè§‚æµ‹é”®: {list(obs.keys())}")
        
        action = env.action_space.sample()
        obs, reward, terminated, truncated, info = env.step(action)
        print(f"  âœ“ step æˆåŠŸ: reward={reward:.4f}, done={terminated}, truncated={truncated}")
        
        env.close()
        print("  âœ“ åº•å±‚å‡½æ•°æµ‹è¯•é€šè¿‡ï¼")
        
    except Exception as e:
        import traceback
        print(f"\n  âœ— åº•å±‚å‡½æ•°æµ‹è¯•å¤±è´¥: {e}")
        print(f"  Traceback:\n{traceback.format_exc()}")
    
    # ====================================================================
    # æ€»ç»“
    # ====================================================================
    print("\n" + "=" * 80)
    print("æµ‹è¯•æ€»ç»“")
    print("=" * 80)
    print("âœ“ æµ‹è¯• 1: RealRobotEnvWrapper + TestRobotEnv - å®Œæˆ")
    print("âš  æµ‹è¯• 2: RealRobotEnvWrapper + RobotEnv - å·²è·³è¿‡ï¼ˆéœ€è¦ç¡¬ä»¶ï¼‰")
    print("âœ“ æµ‹è¯• 3: åº•å±‚å‡½æ•°ï¼ˆmake_robot_env + make_processorsï¼‰- å®Œæˆ")
    print("\nè¯´æ˜:")
    print("  1. TestRobotEnv: æ•°æ®å›æ”¾æ¨¡å¼ï¼Œæ— éœ€çœŸå®ç¡¬ä»¶")
    print("  2. RobotEnv: çœŸå®æœºå™¨äººæ¨¡å¼ï¼Œéœ€è¦é…ç½® robot å’Œ teleop")
    print("  3. RealRobotEnvWrapper: ç»Ÿä¸€æ¥å£ï¼Œå¯åŒæ—¶ä½¿ç”¨ä¸¤ç§æ¨¡å¼")
    print("\nä½¿ç”¨å»ºè®®:")
    print("  - å¼€å‘/æµ‹è¯•: ä½¿ç”¨ TestRobotEnvï¼ˆcfg.env.name='gym_testenv'ï¼‰")
    print("  - ä»¿çœŸè®­ç»ƒ: ä½¿ç”¨ gym_hilï¼ˆcfg.env.name='gym_hil'ï¼‰")
    print("  - çœŸå®éƒ¨ç½²: ä½¿ç”¨ RobotEnvï¼ˆcfg.env.name='real_robot'ï¼Œé…ç½® robot + teleopï¼‰")
    print("=" * 80)