# Copyright 2025 Bytedance Ltd. and/or its affiliates
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.
"""
In single GPU rollout, the sequences are generated directly by sampling from the model.
The output will contain
1. output_ids
2. attention_masks (left padding)
3. eos_masks
4. log_probs
"""

import json
import logging
import os
import inspect  # ===================== CHANGED =====================

import numpy as np
import torch
from PIL import Image
from torch.distributed.fsdp import FullyShardedDataParallel as FSDP
from torch.nn.utils.rnn import pad_sequence
from typing import Any

from recipe.vla.envs.action_utils import center_crop_image, resize_image
from recipe.vla.models.openvla_oft.modeling_prismatic import OpenVLAForActionPrediction
from recipe.vla.models.openvla_oft.processing_prismatic import PrismaticProcessor
from verl import DataProto
from verl.utils.device import get_device_id, get_device_name, get_torch_device
from verl.utils.profiler import simple_timer
from verl.workers.rollout.base import BaseRollout
from recipe.vla.models.pi0_torch.pi0_utils import AlohaInputs, AlohaOutputs

import pdb
logger = logging.getLogger(__name__)

__all__ = ["NaiveRolloutRob", "PI0RolloutRob", "test_pi0_with_lerobot_dataset"]


def pad_sequence_to_length(tensors, max_seq_len, pad_token_id, left_pad=False):
    """
    pad a 2D tensors (e.g. responses, logprobs) in the last dim to max_seq_length.
    input shape: [bs, seq_length]
    output shape: [bs, max_seq_length]
    (0, max_seq_len - tensors.shape[-1]) means right pad to max_seq_length and no left pad
    """
    if tensors.shape[-1] >= max_seq_len:
        return tensors
    pad_tuple = (max_seq_len - tensors.shape[-1], 0) if left_pad else (0, max_seq_len - tensors.shape[-1])
    return torch.nn.functional.pad(tensors, pad_tuple, "constant", pad_token_id)


def process_input(task_descriptions, images_and_states, processor):
    batchdata = {"input_ids": [], "attention_mask": [], "pixel_values": []}

    for i in range(len(task_descriptions)):
        task_description = task_descriptions[i]
        image = resize_image(images_and_states["full_image"][i].cpu().numpy(), (224, 224))
        image = Image.fromarray(image).convert("RGB")
        image = center_crop_image(image)
        prompt = f"In: What action should the robot take to {task_description.lower()}?\nOut:"
        batch_feature = processor(prompt, image)

        input_ids = batch_feature["input_ids"]
        attention_mask = batch_feature["attention_mask"]
        pixel_values = batch_feature["pixel_values"]

        if not torch.all(input_ids[:, -1] == 29871):
            input_ids = torch.cat(
                (input_ids, torch.unsqueeze(torch.Tensor([29871]).long(), dim=0).to(input_ids.device)), dim=1
            )
            attention_mask = torch.cat(
                (attention_mask, torch.unsqueeze(torch.Tensor([True]).bool(), dim=0).to(attention_mask.device)), dim=1
            )

        batchdata["input_ids"].append(input_ids)
        batchdata["attention_mask"].append(attention_mask)
        batchdata["pixel_values"].append(pixel_values)

    device = get_device_id()

    batchdata["input_ids"] = [x.transpose(0, 1) for x in batchdata["input_ids"]]
    batchdata["attention_mask"] = [x.transpose(0, 1) for x in batchdata["attention_mask"]]
    batchdata["input_ids"] = (
        pad_sequence(batchdata["input_ids"], batch_first=True, padding_value=processor.tokenizer.pad_token_id)
        .squeeze(-1)
        .to(device)
    )
    batchdata["attention_mask"] = (
        pad_sequence(batchdata["attention_mask"], batch_first=True, padding_value=0).squeeze(-1).to(device)
    )

    padding_mask = batchdata["input_ids"].ne(processor.tokenizer.pad_token_id)
    assert torch.all(padding_mask == batchdata["attention_mask"].ne(0))
    padding_mask = ~padding_mask
    padding_mask = padding_mask.int()
    sorted_indices = torch.argsort(padding_mask, dim=1, descending=True, stable=True)
    batchdata["input_ids"] = torch.gather(batchdata["input_ids"], 1, sorted_indices)
    batchdata["attention_mask"] = torch.gather(batchdata["attention_mask"], 1, sorted_indices)

    batchdata["pixel_values"] = torch.cat(batchdata["pixel_values"], dim=0).to(device)
    assert torch.all(batchdata["attention_mask"].ne(0) == batchdata["input_ids"].ne(processor.tokenizer.pad_token_id))

    return batchdata


class NaiveRolloutRob(BaseRollout):
    def __init__(
        self,
        model_config: dict,
        module: torch.nn.Module = None,
    ):
        self.model_config = model_config
        if module is not None:
            self.module = module
        else:
            self.module = OpenVLAForActionPrediction.from_pretrained(model_config["path"], trust_remote_code=True)
        self.module.vision_backbone.set_num_images_in_input(1)
        self.processor = PrismaticProcessor.from_pretrained(model_config["path"], trust_remote_code=True)
        dataset_statistics_path = os.path.join(model_config["path"], "dataset_statistics.json")
        if os.path.isfile(dataset_statistics_path):
            with open(dataset_statistics_path) as f:
                norm_stats = json.load(f)
            if isinstance(self.module, FSDP):
                self.module.module.norm_stats = norm_stats
            else:
                self.module.norm_stats = norm_stats
        self.module.eval()

    @torch.no_grad()
    def _generate_one_step(self, prompts: dict, do_sample, temperature, max_prompt_length):
        idx = prompts["input_ids"]  # (bs, prompt_length)
        attention_mask = prompts["attention_mask"]  # left-padded attention_mask
        pixel_values = prompts["pixel_values"]

        with torch.autocast(device_type=get_device_name(), dtype=torch.bfloat16):
            actions, response = self.module.generate_action_verl(
                input_ids=idx,
                pixel_values=pixel_values,
                attention_mask=attention_mask,
                padding_idx=self.processor.tokenizer.pad_token_id,
                do_sample=do_sample,
                unnorm_key="libero_10_no_noops",
                temperature=temperature,
            )

        assert self.processor.tokenizer.pad_token_id is not None

        assert idx.ndim == 2
        idx = pad_sequence_to_length(
            idx, max_seq_len=max_prompt_length, pad_token_id=self.processor.tokenizer.pad_token_id, left_pad=True
        )

        assert attention_mask.ndim == 2
        attention_mask = pad_sequence_to_length(
            attention_mask, max_seq_len=max_prompt_length, pad_token_id=0, left_pad=True
        )

        device_type = get_device_name()
        assert idx.device.type == device_type
        assert response.device.type == device_type
        assert attention_mask.device.type == device_type
        assert pixel_values.device.type == device_type
        batch = {
            "responses": response,
            "input_ids": idx,
            "attention_mask": attention_mask,
            "pixel_values": pixel_values,
            "action": actions,
        }

        return batch



    @torch.no_grad()
    def generate_sequences(self, prompts: DataProto) -> DataProto:
        """Generate sequences"""
        do_sample = prompts.meta_info["do_sample"]
        temperature = prompts.meta_info["temperature"]
        max_prompt_length = prompts.meta_info["prompt_length"]
        task_descriptions = prompts.non_tensor_batch["task_descriptions"]
        images_and_states = {"full_image": prompts.batch["full_image"]}
        vla_input = process_input(task_descriptions, images_and_states, self.processor)

        vla_output = self._generate_one_step(vla_input, do_sample, temperature, max_prompt_length)
        batch = DataProto.from_dict(tensors=vla_output)
        return batch

    async def update_weights(self, weights_iterator, **kwargs):
        prefix = "_fsdp_wrapped_module."
        target_state_dict = self.module.state_dict()
        loaded_tensors_count = 0
        for name, param in weights_iterator:
            cleaned_name = name.replace(prefix, "")
            if cleaned_name in target_state_dict:
                target_tensor = target_state_dict[cleaned_name]
                try:
                    target_tensor.copy_(param, non_blocking=True)
                    loaded_tensors_count += 1
                except Exception as e:
                    logger.warning(f"Warning: Failed to copy tensor '{cleaned_name}'. Error: {e}")
            else:
                logger.warning(f"Warning: Failed to copy tensor '{cleaned_name}'. Model has no such key.")
        logger.info(f"Rollout model weights updated. Loaded {loaded_tensors_count} tensors one by one.")

    async def release(self):
        if self.module.device.type == get_device_name():
            logger.info("Releasing rollout model to CPU.")
            self.module.cpu()
            self.device = torch.device("cpu")
            get_torch_device().empty_cache()

    async def resume(self, **kwargs):
        if self.module.device.type == "cpu":
            target_device = get_device_name()
            logger.info(f"Resuming rollout model to device: {target_device}.")
            self.module.to(target_device)
            self.device = torch.device(target_device)


class PI0RolloutRob(NaiveRolloutRob):
    def __init__(
        self,
        model_config: dict,
        module: torch.nn.Module,
        tokenizer: Any,
    ):
        self.model_config = model_config
        self.module = module
        self.tokenizer = tokenizer
        self.aloha_inputs = AlohaInputs(adapt_to_pi=False)
        self.aloha_outputs = AlohaOutputs(original_action_dim=14, adapt_to_pi=False)
        device = next(module.parameters()).device
        self.aloha_inputs.to(device)
        self.aloha_outputs.to(device)
        # ç”¨äºæµ‹è¯•çš„æ•°æ®é›†ç›¸å…³å˜é‡
        self.test_dataset = None

        # ç”¨äºä¿å­˜è¾“å…¥æ•°æ®çš„å˜é‡
        self.save_inputs_enabled = False
        self.save_inputs_base_path = None
        self.current_step_in_episode = 0
        self._episode_dir = None  # å½“å‰episodeæ–‡ä»¶å¤¹è·¯å¾„

        # ç¡¬ç¼–ç ä¿å­˜è·¯å¾„ï¼Œæ¯æ¬¡åˆ›å»ºç±»æ—¶è‡ªåŠ¨å¯ç”¨ä¿å­˜
        hardcoded_save_path = "/shared_disk/users/weijie.ke/verl/recipe/vla/obs"
        self.enable_input_saving(hardcoded_save_path)

        # ===== TEMP: hardcode load lerobot dataset in rollout worker =====
        try:
            from giga_datasets.datasets.lerobot_dataset import LeRobotDataset
            dataset_path = "/shared_disk/users/yejun.zeng/datasets/huggingface/lerobot/catch_bowl"
            logger.info(f"[TEMP] Auto-loading LeRobot dataset in rollout worker: {dataset_path}")
            self.test_dataset = LeRobotDataset(data_path=dataset_path)
            self.test_dataset.open()
            logger.info(f"[TEMP] Dataset loaded in rollout worker. len={len(self.test_dataset)}")
        except Exception as e:
            logger.exception(f"[TEMP] Failed to auto-load dataset: {e}")
            raise

    def enable_input_saving(self, base_path: str):
        """å¯ç”¨è¾“å…¥æ•°æ®ä¿å­˜åŠŸèƒ½ã€‚åªä¿å­˜æœ€æ–°çš„episodeæ•°æ®ï¼Œæ–°episodeä¼šè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ã€‚"""
        self.save_inputs_enabled = True
        self.save_inputs_base_path = base_path
        self.current_step_in_episode = 0
        self._episode_dir = None
        os.makedirs(base_path, exist_ok=True)
        logger.info(f"å·²å¯ç”¨è¾“å…¥æ•°æ®ä¿å­˜ï¼ˆä»…ä¿å­˜æœ€æ–°episodeï¼‰ï¼Œä¿å­˜è·¯å¾„: {base_path}")

    def disable_input_saving(self):
        """ç¦ç”¨è¾“å…¥æ•°æ®ä¿å­˜åŠŸèƒ½ã€‚"""
        self.save_inputs_enabled = False
        logger.info("å·²ç¦ç”¨è¾“å…¥æ•°æ®ä¿å­˜")

    def _save_inputs(self, cam_high, left_wrist, right_wrist, state, step_idx=None):
        """ä¿å­˜è¾“å…¥å›¾åƒå’ŒçŠ¶æ€åˆ°æ–‡ä»¶ã€‚åªä¿å­˜æœ€æ–°episodeçš„æ•°æ®ï¼Œæ–°episodeä¼šè‡ªåŠ¨è¦†ç›–æ—§æ•°æ®ã€‚"""
        if not self.save_inputs_enabled or self.save_inputs_base_path is None:
            return

        if step_idx is None:
            step_idx = self.current_step_in_episode

        is_new_episode = (self._episode_dir is None) or (step_idx < self.current_step_in_episode)

        if is_new_episode:
            episode_dir = os.path.join(self.save_inputs_base_path, "episode_latest")
            if os.path.exists(episode_dir):
                import shutil
                shutil.rmtree(episode_dir)
            self._episode_dir = episode_dir
            self.current_step_in_episode = 0
            step_idx = 0

        episode_dir = self._episode_dir
        os.makedirs(episode_dir, exist_ok=True)

        image_dir = os.path.join(episode_dir, "image")
        cam_high_dir = os.path.join(image_dir, "cam_high")
        cam_left_wrist_dir = os.path.join(image_dir, "cam_left_wrist")
        cam_right_wrist_dir = os.path.join(image_dir, "cam_right_wrist")
        for dir_path in [cam_high_dir, cam_left_wrist_dir, cam_right_wrist_dir]:
            os.makedirs(dir_path, exist_ok=True)

        batch_size = cam_high.shape[0]

        for b in range(batch_size):
            cam_high_np = cam_high[b].permute(1, 2, 0).cpu().numpy()
            if cam_high_np.max() <= 1.0:
                cam_high_np = (cam_high_np * 255).astype(np.uint8)
            else:
                cam_high_np = cam_high_np.astype(np.uint8)
            Image.fromarray(cam_high_np).save(os.path.join(cam_high_dir, f"step_{step_idx:04d}_batch_{b:02d}.png"))

            left_wrist_np = left_wrist[b].permute(1, 2, 0).cpu().numpy()
            if left_wrist_np.max() <= 1.0:
                left_wrist_np = (left_wrist_np * 255).astype(np.uint8)
            else:
                left_wrist_np = left_wrist_np.astype(np.uint8)
            Image.fromarray(left_wrist_np).save(
                os.path.join(cam_left_wrist_dir, f"step_{step_idx:04d}_batch_{b:02d}.png")
            )

            right_wrist_np = right_wrist[b].permute(1, 2, 0).cpu().numpy()
            if right_wrist_np.max() <= 1.0:
                right_wrist_np = (right_wrist_np * 255).astype(np.uint8)
            else:
                right_wrist_np = right_wrist_np.astype(np.uint8)
            Image.fromarray(right_wrist_np).save(
                os.path.join(cam_right_wrist_dir, f"step_{step_idx:04d}_batch_{b:02d}.png")
            )

        state_file = os.path.join(episode_dir, "state.txt")
        mode = "w" if is_new_episode else "a"
        with open(state_file, mode) as f:
            for b in range(batch_size):
                state_vec = state[b].cpu().numpy()
                state_str = ", ".join([f"{x:.6f}" for x in state_vec])
                f.write(f"step_{step_idx:04d}_batch_{b:02d}: {state_str}\n")

        self.current_step_in_episode += 1

    def _decode_jpeg_images(self, encoded_tensor: torch.Tensor) -> torch.Tensor:
        """è§£ç  JPEG ç¼–ç çš„å›¾åƒæ•°æ®ã€‚"""
        import cv2
        import numpy as np

        batch_size = encoded_tensor.shape[0]
        decoded_images = []

        for i in range(batch_size):
            img_bytes = encoded_tensor[i].cpu().numpy().tobytes()
            nparr = np.frombuffer(img_bytes, np.uint8)
            img = cv2.imdecode(nparr, cv2.IMREAD_COLOR)

            if img is not None:
                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)
                decoded_images.append(torch.from_numpy(img_rgb))
            else:
                decoded_images.append(torch.zeros((224, 224, 3), dtype=torch.uint8))

        return torch.stack(decoded_images)

    def load_test_dataset(self, dataset_path: str):
        """åŠ è½½æµ‹è¯•æ•°æ®é›†ã€‚"""
        from giga_datasets.datasets.lerobot_dataset import LeRobotDataset

        logger.info(f"åŠ è½½ LeRobot æ•°æ®é›†: {dataset_path}")
        self.test_dataset = LeRobotDataset(data_path=dataset_path)
        self.test_dataset.open()
        logger.info(f"æ•°æ®é›†åŠ è½½å®Œæˆï¼Œå…± {len(self.test_dataset)} ä¸ªæ ·æœ¬")

    # ===================== CHANGED: robust meta.episodes parser =====================
    def _episode_lengths_from_meta(self, meta) -> list[int]:
        eps = meta.episodes
        if isinstance(eps, list):
            lengths = []
            for ep in eps:
                if isinstance(ep, dict) and "length" in ep:
                    lengths.append(int(ep["length"]))
                else:
                    lengths.append(int(ep))
            return lengths

        if isinstance(eps, dict):
            # common cases:
            # 1) {"length": [..]}
            if "length" in eps and isinstance(eps["length"], (list, tuple)):
                return [int(x) for x in eps["length"]]

            # 2) {"0": {"length": ..}, "1": {"length": ..}, ...} OR other sortable keys
            items = list(eps.items())
            try:
                items = sorted(items, key=lambda kv: int(kv[0]))
            except Exception:
                items = sorted(items, key=lambda kv: str(kv[0]))
            lengths = []
            for _, v in items:
                if isinstance(v, dict) and "length" in v:
                    lengths.append(int(v["length"]))
                else:
                    lengths.append(int(v))
            return lengths

        raise TypeError(f"Unsupported meta.episodes type: {type(eps)}")

    def _episode_start_index(self, meta, episode_idx: int) -> tuple[int, int]:
        lengths = self._episode_lengths_from_meta(meta)
        if episode_idx < 0 or episode_idx >= len(lengths):
            raise IndexError(f"episode_idx out of range: {episode_idx} not in [0, {len(lengths)-1}]")
        start = int(sum(lengths[:episode_idx]))
        return start, int(lengths[episode_idx])
    # ===================== END CHANGED =====================

    # ===================== CHANGED: chunked episode fetch =====================
    def get_episode_chunk(
        self,
        episode_idx: int,
        start_step: int,
        chunk_len: int,
        *,
        action_dim: int,
    ):
        """
        ä»æ•°æ®é›†ä¸­è·å–ä¸€ä¸ª chunk è¾“å…¥ï¼ˆ1å¸§ obsï¼‰ + åç»­ chunk_len ä¸ª GT absolute actionsã€‚

        æ•°æ®é›†çº¦å®šï¼ˆæ ¹æ®DEBUGåˆ†æç¡®è®¤ï¼‰ï¼š
          - sample["observation.state"] æ˜¯å½“å‰å¸§ state[t]ï¼ˆç»å¯¹å…³èŠ‚ + ç»å¯¹gripperï¼‰
          - sample["action"] æ˜¯ä¸‹ä¸€æ­¥çš„ç»å¯¹ç›®æ ‡ä½ç½®ï¼ˆä¸æ˜¯deltaï¼ï¼‰
              * éªŒè¯ï¼šaction[t] â‰ˆ state[t+1]
          - GT = actionï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦è½¬æ¢ï¼‰
        
        æ¨¡å‹è¾“å‡ºçº¦å®šï¼š
          - æ¨¡å‹è¾“å‡º = ç»å¯¹å…³èŠ‚ + ç»å¯¹gripper
        """
        if self.test_dataset is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_test_dataset() åŠ è½½æ•°æ®é›†")

        meta = self.test_dataset.dataset.meta
        episode_start_idx, episode_length = self._episode_start_index(meta, episode_idx)

        # å¿…é¡»ä¿è¯è¿˜æœ‰ chunk_len ä¸ª action
        if start_step < 0 or start_step >= episode_length:
            raise ValueError(f"start_step out of range: {start_step} not in [0, {episode_length-1}]")
        if start_step + chunk_len > episode_length:
            # ä¸è¶³ chunk_lenï¼šæŒ‰ä½ çš„è¦æ±‚ç›´æ¥åœæ­¢ï¼ˆè°ƒç”¨æ–¹ breakï¼‰
            return None

        # input sample at t=start_step
        sample0 = self.test_dataset[episode_start_idx + start_step]

        head_img0 = sample0["observation.images.cam_high"].permute(1, 2, 0).contiguous()
        left_wrist_img0 = sample0["observation.images.cam_left_wrist"].permute(1, 2, 0).contiguous()
        right_wrist_img0 = sample0["observation.images.cam_right_wrist"].permute(1, 2, 0).contiguous()

        state0 = sample0["observation.state"].clone()[:action_dim]  # [A] absolute current state
        
        # å·¦è‡‚ï¼š0-5 å…³èŠ‚(delta)ï¼Œ6 å¤¹çˆª(ç»å¯¹)
        # å³è‡‚ï¼š7-12 å…³èŠ‚(delta)ï¼Œ13 å¤¹çˆª(ç»å¯¹)
        joint_dims = list(range(6)) + list(range(7, 13)) if action_dim >= 14 else list(range(6))
        gripper_dims = [6, 13] if action_dim >= 14 else [6]
        
        gt_abs = []
        for k in range(chunk_len):
            s = self.test_dataset[episode_start_idx + start_step + k]
            state_t = s["observation.state"].clone()[:action_dim]  # [A] å½“å‰æ—¶é—´æ­¥çš„stateï¼ˆç»å¯¹å…³èŠ‚+ç»å¯¹gripperï¼‰
            raw_action = s["action"].clone()[:action_dim]  # æ•°æ®é›†åŸå§‹actionï¼ˆdelta joint + ç»å¯¹gripperï¼‰
            
            # ğŸ”§ DEBUG: æ‰“å°å‰3ä¸ªæ—¶é—´æ­¥çš„åŸå§‹æ•°æ®ï¼ˆchunk 0å’Œchunk 1ï¼‰
            if start_step <= 30 and k < 3:
                print(f"\n[DEBUG RAW DATA] k={k}:")
                print(f"  state_t[:7]:        {state_t[:7]}")
                print(f"  raw_action[:7]:     {raw_action[:7]}")
                print(f"  state0 + raw:       {state0[:7] + raw_action[:7]}")
                print(f"  state_t + raw:      {state_t[:7] + raw_action[:7]}")
            
            # ===================== GTè½¬æ¢é€»è¾‘ =====================
            # æ ¹æ®DEBUGè¾“å‡ºåˆ†æï¼š
            #   - state[t]: ç»å¯¹å…³èŠ‚ + ç»å¯¹gripper
            #   - action[t]: å°±æ˜¯ä¸‹ä¸€æ­¥çš„ç»å¯¹ç›®æ ‡ä½ç½®ï¼ˆä¸æ˜¯deltaï¼ï¼‰
            #   - éªŒè¯ï¼šraw_action[k=0] â‰ˆ state_t[k=1]
            # 
            # æ¨¡å‹è¾“å‡ºæ ¼å¼ï¼š
            #   - ç»å¯¹å…³èŠ‚ + ç»å¯¹gripper
            # 
            # å› æ­¤GT = raw_actionï¼ˆç›´æ¥ä½¿ç”¨ï¼Œä¸éœ€è¦è½¬æ¢ï¼‰
            abs_action = raw_action.clone()
            # ä¸éœ€è¦ä»»ä½•è½¬æ¢ï¼Œactionæœ¬èº«å°±æ˜¯ç»å¯¹ç›®æ ‡ä½ç½®
            
            gt_abs.append(abs_action)
        gt_abs = torch.stack(gt_abs, dim=0)  # [T,A]

        task = sample0["task"]
        return {
            "head_image": head_img0,                   # [H,W,C]
            "left_wrist_image": left_wrist_img0,       # [H,W,C]
            "right_wrist_image": right_wrist_img0,     # [H,W,C]
            "state": state0,                           # [A]
            "action_abs_seq": gt_abs,                  # [T,A] - å·²è½¬æ¢ä¸ºç»å¯¹å€¼
            "task": task,
            "episode_idx": episode_idx,
            "start_step": start_step,
            "episode_length": episode_length,
        }
    # ===================== END CHANGED =====================
    # @torch.no_grad()
    # def generate_sequences(self, prompts: DataProto) -> DataProto:
    #     """
    #     TEMP HACK (real-robot debug):
    #     Ignore real robot inputs; use LeRobot dataset obs instead.
    #     Dirty but stable.
    #     """
    #     # -----------------------------
    #     # hardcode episode control
    #     EPISODE_IDX = 0
    #     START_STEP = 0
    #     STRIDE = 10  # æ¯æ¬¡è°ƒç”¨æ¨è¿›å¤šå°‘æ­¥ï¼ˆä½ æƒ³ 1 å°±æ”¹ 1ï¼‰

    #     # è®°å½•å…¨å±€ stepï¼ˆå‡½æ•°é™æ€å˜é‡ï¼Œä¸æ±¡æŸ“ selfï¼‰
    #     if not hasattr(PI0RolloutRob.generate_sequences, "_ds_step"):
    #         PI0RolloutRob.generate_sequences._ds_step = START_STEP

    #     if self.test_dataset is None:
    #         raise RuntimeError("[TEMP] test_dataset is None. Your __init__ should auto-load it, or call load_test_dataset().")

    #     # å½“å‰ episode é•¿åº¦ï¼Œç”¨äº wrap
    #     meta = self.test_dataset.dataset.meta
    #     _, episode_length = self._episode_start_index(meta, EPISODE_IDX)

    #     # ç”¨ prompts çš„ batch sizeï¼ˆçœŸæœºå¯èƒ½ != 1ï¼‰
    #     # å°½é‡ä» head_image æ¨æ–­ï¼›æ²¡æœ‰å°± fallback 1
    #     try:
    #         B = int(prompts.batch["head_image"].shape[0])
    #     except Exception:
    #         B = 1

    #     ds_step0 = int(PI0RolloutRob.generate_sequences._ds_step)
    #     print(f"[TEMP] ds_step={ds_step0}, B={B}, STRIDE={STRIDE}, episode_len={episode_length}")

    #     # -----------------------------
    #     # ä» dataset æ„é€  B ä¸ªæ ·æœ¬ï¼ˆæ¯ä¸ªæ ·æœ¬å–ä¸€å¸§ obsï¼Œtask å¯ä»¥ç›¸åŒï¼‰
    #     heads, lefts, rights, states = [], [], [], []
    #     tasks = []

    #     for b in range(B):
    #         st = ds_step0 + b  # åŒä¸€æ¬¡è°ƒç”¨å†…ç”¨ç›¸é‚»å¸§ï¼Œé¿å…å…¨éƒ½ä¸€æ ·
    #         # wrap
    #         st = st % max(1, episode_length)

    #         chunk = self.get_episode_chunk(
    #             episode_idx=EPISODE_IDX,
    #             start_step=st,
    #             chunk_len=1,          # åªè¦ 1 å¸§ obsï¼ˆGT action æˆ‘ä»¬åªåšæ‰“å°ç”¨ï¼‰
    #             action_dim=14,        # ä½ çš„ action_dim
    #         )
    #         if chunk is None:
    #             # ç†è®ºä¸Šä¸ä¼šå‘ç”Ÿï¼ˆchunk_len=1, wrap åè‚¯å®šå¤Ÿï¼‰
    #             raise RuntimeError(f"[TEMP] get_episode_chunk returned None at step={st}")

    #         heads.append(chunk["head_image"])             # [H,W,C]
    #         lefts.append(chunk["left_wrist_image"])
    #         rights.append(chunk["right_wrist_image"])
    #         states.append(chunk["state"])                 # [A]
    #         tasks.append(chunk["task"])

    #     # æ¨è¿›å…¨å±€æ­¥æ•°ï¼ˆä¸€æ¬¡è°ƒç”¨æ¨è¿› STRIDEï¼‰
    #     PI0RolloutRob.generate_sequences._ds_step = (ds_step0 + STRIDE) % max(1, episode_length)

    #     head_image = torch.stack(heads, dim=0)            # [B,H,W,C]
    #     left_wrist_image = torch.stack(lefts, dim=0)
    #     right_wrist_image = torch.stack(rights, dim=0)
    #     state = torch.stack(states, dim=0)                # [B,A]
    #     task_descriptions = np.array(tasks)               # len=B

    #     # -----------------------------
    #     # Below: keep your original logic as-is
    #     timing_generate = {}
    #     with simple_timer("rollout generate_sequences", timing_generate):

    #         # 1) ç»Ÿä¸€ state shape
    #         if state.ndim == 3:
    #             state = state[:, -1, :]
    #         elif state.ndim == 2:
    #             pass
    #         elif state.ndim == 1:
    #             state = state.unsqueeze(0)
    #         else:
    #             raise ValueError(f"[PI0RolloutRob] Unexpected state shape: {state.shape}")

    #         device = next(self.module.parameters()).device
    #         state = state.to(device=device, dtype=torch.float32)
    #         raw_state_dim = int(state.shape[-1])

    #         # pad åˆ° 32
    #         state_pad32 = torch.nn.functional.pad(state, (0, max(0, 32 - raw_state_dim)), "constant", 0.0)

    #         sample_sig = inspect.signature(self.module.sample_actions)
    #         supports_state_dim = "state_dim" in sample_sig.parameters

    #         with torch.autocast(device_type=get_device_name(), dtype=torch.bfloat16):
    #             # dataset æ˜¯ [B,H,W,C]ï¼Œä¸ä¼šèµ° jpeg decode åˆ†æ”¯ï¼›ä¿ç•™ä¹Ÿæ— å¦¨
    #             if head_image.ndim == 2:
    #                 head_image = self._decode_jpeg_images(head_image)
    #             if left_wrist_image.ndim == 2:
    #                 left_wrist_image = self._decode_jpeg_images(left_wrist_image)
    #             if right_wrist_image.ndim == 2:
    #                 right_wrist_image = self._decode_jpeg_images(right_wrist_image)

    #             batch_size = head_image.shape[0]
    #             cam_high = head_image.permute(0, 3, 1, 2).to(device)
    #             left_wrist = left_wrist_image.permute(0, 3, 1, 2).to(device)
    #             right_wrist = right_wrist_image.permute(0, 3, 1, 2).to(device)

    #             kwargs = dict(
    #                 images={
    #                     "observation.images.cam_high": cam_high,
    #                     "observation.images.cam_left_wrist": left_wrist,
    #                     "observation.images.cam_right_wrist": right_wrist,
    #                 },
    #                 img_masks=[
    #                     torch.ones((batch_size,), device=device, dtype=torch.bool),
    #                     torch.ones((batch_size,), device=device, dtype=torch.bool),
    #                     torch.ones((batch_size,), device=device, dtype=torch.bool),
    #                 ],
    #                 task=task_descriptions.tolist(),
    #                 state=state_pad32,
    #                 tokenizer=self.tokenizer,
    #             )
    #             if supports_state_dim:
    #                 kwargs["state_dim"] = raw_state_dim

    #             if getattr(self, "save_inputs_enabled", False):
    #                 self._save_inputs(cam_high, left_wrist, right_wrist, state_pad32)

    #             (
    #                 action,
    #                 images_out,
    #                 img_masks,
    #                 lang_tokens,
    #                 lang_masks,
    #                 state_out,
    #             ) = self.module.sample_actions(**kwargs)

    #     print("rollout generate_sequences time (s): %s" % timing_generate.get("rollout generate_sequences", 0.0))

    #     # chunk_len/action_dim
    #     cfg = getattr(self.module, "config", None)
    #     T = getattr(cfg, "num_action_chunks", 10)
    #     A = getattr(cfg, "action_dim", action.shape[-1])
    #     T = min(int(T), int(action.shape[1]))
    #     A = min(int(A), int(action.shape[2]))

    #     # -----------------------------
    #     # TEMP PRINT: pred[0,0] vs GT(step0)ï¼ˆç”¨ç¬¬ä¸€æ¡æ ·æœ¬åš quick sanityï¼‰
    #     try:
    #         # é‡æ–°å–ä¸€ä¸‹ç¬¬ä¸€æ¡çš„ GTï¼šaction_abs_seq[0] æ˜¯ gt_abs at step=st
    #         st0 = ds_step0 % max(1, episode_length)
    #         chunk0 = self.get_episode_chunk(EPISODE_IDX, st0, 1, action_dim=A)
    #         gt_abs0 = chunk0["action_abs_seq"][0].to(dtype=torch.float32)[:A]          # [A]
    #         pred0 = action[0, 0, :A].to(dtype=torch.float32).detach().cpu()            # [A]ï¼ˆæ³¨æ„ï¼šè¿™é‡Œ pred è¿˜æ˜¯æ¨¡å‹åŸå§‹è¯­ä¹‰ï¼‰
    #         diff = (pred0 - gt_abs0.cpu())
    #         print("\n[TEMP] pred0 vs gt_abs0 (first sample, first step)")
    #         print("  pred0 :", "[" + ", ".join([f"{x:.6f}" for x in pred0.tolist()]) + "]")
    #         print("  gt_abs:", "[" + ", ".join([f"{x:.6f}" for x in gt_abs0.cpu().tolist()]) + "]")
    #         print("  |diff| mean=%.6f max=%.6f\n" % (diff.abs().mean().item(), diff.abs().max().item()))
    #     except Exception as e:
    #         print(f"[TEMP] skip pred-vs-gt print due to: {e}")

    #     ret = DataProto.from_dict(
    #         {
    #             "action": action[:, :T, :A],
    #             "full_action": action,
    #             "images": torch.stack(images_out, dim=1) if isinstance(images_out, (list, tuple)) else images_out,
    #             "image_masks": torch.stack(img_masks, dim=1) if isinstance(img_masks, (list, tuple)) else img_masks,
    #             "lang_tokens": lang_tokens,
    #             "lang_masks": lang_masks,
    #             "states": state_out,
    #         }
    #     )
    #     return ret

    @torch.no_grad()
    def generate_sequences(self, prompts: DataProto) -> DataProto:
        """Generate sequences"""
        head_image = prompts.batch["head_image"]
        left_wrist_image = prompts.batch["left_wrist_image"]
        right_wrist_image = prompts.batch["right_wrist_image"]
        state = prompts.batch["state"]
        task_descriptions = prompts.non_tensor_batch["task_descriptions"]

        timing_generate = {}
        with simple_timer("rollout generate_sequences", timing_generate):

            # 1) ç»Ÿä¸€ state shape
            if state.ndim == 3:
                state = state[:, -1, :]
            elif state.ndim == 2:
                pass
            elif state.ndim == 1:
                state = state.unsqueeze(0)
            else:
                raise ValueError(f"[PI0RolloutRob] Unexpected state shape: {state.shape}")

            device = prompts.batch.device
            state = state.to(device=device, dtype=torch.float32)
            raw_state_dim = int(state.shape[-1])  # usually action_dim (e.g., 14)

            # prompt ç”¨ raw_state_dimï¼ˆä¸å« padï¼‰ï¼Œæ¨¡å‹ forward ç”¨ pad32
            state_pad32 = torch.nn.functional.pad(state, (0, max(0, 32 - raw_state_dim)), "constant", 0.0)
            state_pad32 = state_pad32.to(device=device, dtype=torch.float32)
            sample_sig = inspect.signature(self.module.sample_actions)
            supports_state_dim = "state_dim" in sample_sig.parameters

            with torch.autocast(device_type=get_device_name(), dtype=torch.bfloat16):
                if head_image.ndim == 2:
                    head_image = self._decode_jpeg_images(head_image)
                if left_wrist_image.ndim == 2:
                    left_wrist_image = self._decode_jpeg_images(left_wrist_image)
                if right_wrist_image.ndim == 2:
                    right_wrist_image = self._decode_jpeg_images(right_wrist_image)

                batch_size = head_image.shape[0]
                cam_high = head_image.permute(0, 3, 1, 2).to(device)
                left_wrist = left_wrist_image.permute(0, 3, 1, 2).to(device)
                right_wrist = right_wrist_image.permute(0, 3, 1, 2).to(device)

                kwargs = dict(
                    images={
                        "observation.images.cam_high": cam_high,
                        "observation.images.cam_left_wrist": left_wrist,
                        "observation.images.cam_right_wrist": right_wrist,
                    },
                    img_masks=[
                        torch.ones((batch_size,), device=device, dtype=torch.bool),
                        torch.ones((batch_size,), device=device, dtype=torch.bool),
                        torch.ones((batch_size,), device=device, dtype=torch.bool),
                    ],
                    task=task_descriptions.tolist() if hasattr(task_descriptions, "tolist") else list(task_descriptions),
                    state=state_pad32,
                    tokenizer=self.tokenizer,
                )

                if supports_state_dim:
                    kwargs["state_dim"] = raw_state_dim
                
                # ===================== CHANGED =====================
                # ä»æ¨¡å‹configä¸­è¯»å–use_endposeå’Œno_stateé…ç½®å¹¶ä¼ é€’
                cfg = getattr(self.module, "config", None)
                if cfg is not None:
                    use_endpose = getattr(cfg, "use_endpose", False)
                    no_state = getattr(cfg, "no_state", False)
                    kwargs["use_endpose"] = use_endpose
                    kwargs["no_state"] = no_state
                # ===================== END CHANGED =====================

                if self.save_inputs_enabled:
                    self._save_inputs(cam_high, left_wrist, right_wrist, state_pad32)

                (
                    action,
                    images_out,
                    img_masks,
                    lang_tokens,
                    lang_masks,
                    state_out,
                ) = self.module.sample_actions(**kwargs)
        print("rollout generate_sequences time (s): %s" % timing_generate.get("rollout generate_sequences", 0.0))

        # ===================== CHANGED =====================
        # chunk_len é»˜è®¤ 30ï¼ˆæŒ‰ä½ çš„è¦æ±‚ï¼‰ï¼Œå–ä¸åˆ°å°± fallback=30
        cfg = getattr(self.module, "config", None)
        T = getattr(cfg, "num_action_chunks", 30)
        A = getattr(cfg, "action_dim", action.shape[-1])
        T = min(int(T), int(action.shape[1]))
        A = min(int(A), int(action.shape[2]))
        # ===================== END CHANGED =====================

        ret = DataProto.from_dict(
            {
                "action": action[:, :T, :A],
                "full_action": action,
                "images": torch.stack(images_out, dim=1) if isinstance(images_out, (list, tuple)) else images_out,
                "image_masks": torch.stack(img_masks, dim=1) if isinstance(img_masks, (list, tuple)) else img_masks,
                "lang_tokens": lang_tokens,
                "lang_masks": lang_masks,
                "states": state_out,
            }
        )
        return ret

    # ===================== CHANGED: episode evaluation (chunked, 30-step) =====================
    @torch.no_grad()
    def test_episode_chunked(
        self,
        episode_idx: int = 0,
        start_step: int = 0,
        *,
        max_chunks: int | None = None,
        verbose: bool = True,
        test_fk_conversion: bool = False,
    ):
        """
        å¯¹ä¸€æ®µ episode åš chunk è¯„ä¼°ï¼š
          - æ¯æ¬¡ç”¨ 1 å¼ å›¾ç‰‡/1 ä¸ª state è¾“å…¥æ¨¡å‹
          - æ¨¡å‹è¾“å‡ºä¸€ä¸ª action chunkï¼ˆé»˜è®¤ 30 stepsï¼Œå·²å¯¹é½ä¸º ABSï¼‰
          - ç”¨æ•°æ®é›†å¯¹åº”çš„åç»­ 30 ä¸ª absolute action ä½œä¸º GT
          - å¯¹æ¯ä¸ª chunkï¼Œåˆ†åˆ«ç»Ÿè®¡ï¼š
              - å‰10æ­¥ / ä¸­10æ­¥ / å10æ­¥ çš„è¯¯å·®
              - joints vs grippers çš„è¯¯å·®ï¼ˆåˆ†å¼€ï¼‰
          - episode æœ«å°¾ä¸è¶³ 30 æ­¥ï¼šç›´æ¥åœæ­¢
          - æœ€åè¾“å‡º episode æ€»ä½“ joints/grippers è¯¯å·®
        
        Args:
            test_fk_conversion: å¦‚æœä¸ºTrueï¼Œå°†joint actioné€šè¿‡FKè½¬æ¢ä¸ºendposeè¿›è¡Œå¯¹æ¯”æµ‹è¯•
        """
        if self.test_dataset is None:
            raise ValueError("è¯·å…ˆè°ƒç”¨ load_test_dataset() åŠ è½½æ•°æ®é›†")
        
        # ===================== FKè½¬æ¢åˆå§‹åŒ– =====================
        print(f"\n[FK Test] test_fk_conversionå‚æ•°: {test_fk_conversion}")
        left_kin = None
        right_kin = None
        _rotmat_to_rpy_zyx = None
        
        if test_fk_conversion:
            print("[FK Test] å¼€å§‹åˆå§‹åŒ–è¿åŠ¨å­¦æ±‚è§£å™¨...")
            try:
                import sys
                piper_path = "/shared_disk/users/weijie.ke/verl/recipe/vla/envs/robot_env/robot/controller/piper"
                if piper_path not in sys.path:
                    sys.path.insert(0, piper_path)
                
                # å¯¼å…¥æœ¬åœ°çš„lerobotæ¨¡å—ï¼ˆä¸æ˜¯pipå®‰è£…çš„ï¼‰
                from lerobot.model.kinematics import RobotKinematics
                
                # ç›´æ¥å®šä¹‰æ—‹è½¬çŸ©é˜µè½¬RPYå‡½æ•°ï¼Œé¿å…å¯¼å…¥é—®é¢˜
                def _rotmat_to_rpy_zyx(R: np.ndarray) -> np.ndarray:
                    """å°†æ—‹è½¬çŸ©é˜µè½¬æ¢ä¸ºRPYï¼ˆRoll-Pitch-Yawï¼‰æ¬§æ‹‰è§’ï¼ˆZYXé¡ºåºï¼‰"""
                    r20 = -R[2, 0]
                    r20_clamped = float(np.clip(r20, -1.0, 1.0))
                    pitch = np.arcsin(r20_clamped)
                    
                    cos_pitch = np.cos(pitch)
                    if abs(cos_pitch) < 1e-6:
                        # é€€åŒ–æƒ…å†µï¼ˆæ¥è¿‘ Â±90Â°ï¼‰
                        roll = 0.0
                        yaw = np.arctan2(-R[0, 1], R[1, 1])
                    else:
                        roll = np.arctan2(R[2, 1], R[2, 2])
                        yaw = np.arctan2(R[1, 0], R[0, 0])
                    
                    return np.array([float(roll), float(pitch), float(yaw)], dtype=float)
                
                urdf_path = "/shared_disk/users/weijie.ke/verl/recipe/vla/envs/robot_env/robot/controller/piper/local_assets/robot.urdf"
                print(f"[FK Test] åŠ è½½URDF: {urdf_path}")
                print("[FK Test] æ³¨æ„ï¼šURDFä¸­å…³èŠ‚åä¸º joint1-joint8ï¼ˆæ— left/rightå‰ç¼€ï¼‰")
                print("[FK Test] å‡è®¾ joint1-joint6 ä¸ºå·¦è‡‚ï¼Œéœ€è¦ä¸ºå³è‡‚åˆ›å»ºå•ç‹¬çš„URDFæˆ–é•œåƒå¤„ç†")
                
                # å·¦è‡‚ï¼šä½¿ç”¨ joint1-joint6
                left_kin = RobotKinematics(
                    urdf_path=urdf_path,
                    target_frame_name="link6",  # æœ«ç«¯æ‰§è¡Œå™¨link
                    joint_names=[
                        "joint1",
                        "joint2",
                        "joint3",
                        "joint4",
                        "joint5",
                        "joint6",
                    ],
                )
                print("[FK Test] å·¦è‡‚è¿åŠ¨å­¦æ±‚è§£å™¨åˆå§‹åŒ–æˆåŠŸ (joint1-joint6)")
                
                # å³è‡‚ï¼šæš‚æ—¶ä½¿ç”¨ç›¸åŒçš„URDFï¼ˆåº”è¯¥æ˜¯é•œåƒé…ç½®ï¼‰
                # TODO: å¦‚æœæœ‰å•ç‹¬çš„å³è‡‚URDFï¼Œåº”è¯¥ä½¿ç”¨ä¸åŒçš„æ–‡ä»¶
                right_kin = RobotKinematics(
                    urdf_path=urdf_path,
                    target_frame_name="link6",
                    joint_names=[
                        "joint1",  # å³è‡‚ä¹Ÿæ˜¯6ä¸ªå…³èŠ‚ï¼Œä½†åœ¨åŒè‡‚ç³»ç»Ÿä¸­éœ€è¦ä¸åŒå¤„ç†
                        "joint2",
                        "joint3",
                        "joint4",
                        "joint5",
                        "joint6",
                    ],
                )
                print("[FK Test] å³è‡‚è¿åŠ¨å­¦æ±‚è§£å™¨åˆå§‹åŒ–æˆåŠŸ")
                print("[FK Test] âœ“ è¿åŠ¨å­¦æ±‚è§£å™¨åˆå§‹åŒ–å®Œæˆï¼ŒFKè½¬æ¢å·²å¯ç”¨\n")
            except Exception as e:
                import traceback
                print(f"[FK Test] âœ— è¿åŠ¨å­¦æ±‚è§£å™¨åˆå§‹åŒ–å¤±è´¥: {e}")
                print(f"[FK Test] Traceback:\n{traceback.format_exc()}")
                test_fk_conversion = False
        else:
            print("[FK Test] FKè½¬æ¢æœªå¯ç”¨ï¼ˆtest_fk_conversion=Falseï¼‰\n")
        # ===================== END FKè½¬æ¢åˆå§‹åŒ– =====================

        cfg = getattr(self.module, "config", None)
        chunk_len = int(getattr(cfg, "num_action_chunks", 30))
        action_dim = int(getattr(cfg, "action_dim", 14))

        if chunk_len != 30:
            logger.warning(f"[chunk-eval] config.num_action_chunks={chunk_len} (expected 30). Still proceed.")

        # ===================== åªè¯„ä¼°å·¦è‡‚ï¼ˆå‰7ç»´ï¼‰=====================
        # å·¦è‡‚ï¼š0-5 å…³èŠ‚ï¼Œ6 å¤¹çˆª
        # å³è‡‚ï¼š7-12 å…³èŠ‚ï¼Œ13 å¤¹çˆªï¼ˆå¿½ç•¥ï¼‰
        eval_dim = 7  # åªè¯„ä¼°å·¦è‡‚
        gripper_idx = [6]  # åªè¯„ä¼°å·¦è‡‚å¤¹çˆª
        joint_idx = [i for i in range(eval_dim) if i not in gripper_idx]  # 0-5

        # segments: first/middle/last 10 (only meaningful for 30)
        def _segments(T: int):
            if T >= 30:
                return [
                    ("first10", slice(0, 10)),
                    ("mid10", slice(10, 20)),
                    ("last10", slice(20, 30)),
                ]
            # fallback: split into 3 parts
            k = T // 3
            return [
                ("first", slice(0, k)),
                ("mid", slice(k, 2 * k)),
                ("last", slice(2 * k, T)),
            ]

        # accumulate episode totals
        ep_joint_abs_sum = 0.0
        ep_joint_count = 0
        ep_grip_abs_sum = 0.0
        ep_grip_count = 0

        chunk_id = 0
        cur = int(start_step)

        # episode length for stopping (robust)
        meta = self.test_dataset.dataset.meta
        _, episode_length = self._episode_start_index(meta, episode_idx)

        print("\n" + "=" * 100)
        print(f"[EPISODE] idx={episode_idx}  start_step={start_step}  episode_length={episode_length}")
        print(f"[CONFIG] chunk_len={chunk_len}  eval_dim={eval_dim} (å·¦è‡‚)  gripper_idx={gripper_idx}  joint_dims={len(joint_idx)}")
        print("=" * 100)

        while True:
            if max_chunks is not None and chunk_id >= int(max_chunks):
                break

            chunk = self.get_episode_chunk(
                episode_idx=episode_idx,
                start_step=cur,
                chunk_len=chunk_len,
                action_dim=action_dim,
            )
            if chunk is None:
                print(f"\n[STOP] remaining steps < chunk_len ({chunk_len}). stop at step={cur}.")
                break

            # build prompts: ONLY ONE IMAGE for the chunk
            device = next(self.module.parameters()).device
            head_img = chunk["head_image"].unsqueeze(0).to(device)          # [1,H,W,C]
            left_img = chunk["left_wrist_image"].unsqueeze(0).to(device)
            right_img = chunk["right_wrist_image"].unsqueeze(0).to(device)
            state0 = chunk["state"].unsqueeze(0).to(device)                 # [1,A]
            task = chunk["task"]

            prompts = DataProto.from_dict(
                tensors={
                    "head_image": head_img,
                    "left_wrist_image": left_img,
                    "right_wrist_image": right_img,
                    "state": state0,
                },
                non_tensors={"task_descriptions": np.array([task])},
            )

            out = self.generate_sequences(prompts)
            pred_full = out.batch["action"].detach().float().cpu()          # [1,T,A] æ¨¡å‹è¾“å‡º
            gt_full_joint = chunk["action_abs_seq"].detach().float().cpu()  # [T,A] æ•°æ®é›†GTï¼ˆç»å¯¹jointï¼‰

            T = min(int(pred_full.shape[1]), int(gt_full_joint.shape[0]), 30 if chunk_len >= 30 else chunk_len)
            
            # ===================== FKè½¬æ¢ï¼šå°†GTçš„jointè½¬ä¸ºendpose =====================
            if test_fk_conversion and left_kin is not None and right_kin is not None and _rotmat_to_rpy_zyx is not None:
                # å…³é”®ï¼šå°†GTçš„joint actioné€šè¿‡FKè½¬æ¢ä¸ºendposeï¼Œç„¶åä¸æ¨¡å‹é¢„æµ‹çš„endposeå¯¹æ¯”
                # æ³¨æ„ï¼šendposeå­¦ä¹ çš„æ˜¯ç›¸å¯¹å˜åŒ–ï¼ˆdeltaï¼‰ï¼Œgripperå­¦ä¹ çš„æ˜¯ç»å¯¹å€¼
                print("\n[FK Test] å°†GTçš„jointé€šè¿‡FKè½¬æ¢ä¸ºendposeï¼ˆç›¸å¯¹å˜åŒ–ï¼‰...")
                
                # 0. é¦–å…ˆè®¡ç®—state0çš„FKï¼ˆä½œä¸ºåŸºå‡†ï¼‰
                state0_joints = state0.cpu().squeeze()[:14].numpy()  # [14] å½“å‰stateçš„jointè§’åº¦
                
                # å·¦è‡‚state FK
                ql_state_rad = state0_joints[:6].astype(float)
                ql_state_deg = np.rad2deg(ql_state_rad)
                T_l_state = left_kin.forward_kinematics(ql_state_deg)
                p_l_state = T_l_state[:3, 3]
                rpy_l_state = _rotmat_to_rpy_zyx(T_l_state[:3, :3])
                
                # å³è‡‚state FK
                qr_state_rad = state0_joints[7:13].astype(float)
                qr_state_deg = np.rad2deg(qr_state_rad)
                T_r_state = right_kin.forward_kinematics(qr_state_deg)
                p_r_state = T_r_state[:3, 3]
                rpy_r_state = _rotmat_to_rpy_zyx(T_r_state[:3, :3])
                
                print(f"[FK Test] State0 å·¦è‡‚endpose: pos={p_l_state}, rpy={rpy_l_state}")
                print(f"[FK Test] State0 å³è‡‚endpose: pos={p_r_state}, rpy={rpy_r_state}")
                
                # 1. è½¬æ¢GTçš„joint action -> endpose deltaï¼ˆç›¸å¯¹äºstate0ï¼‰
                gt_endpose_list = []
                for t in range(T):
                    joint_action = gt_full_joint[t, :14].numpy()  # [14] ç»å¯¹jointè§’åº¦(rad)
                    
                    # å·¦è‡‚FK
                    ql_rad = joint_action[:6].astype(float)
                    ql_deg = np.rad2deg(ql_rad)
                    T_l = left_kin.forward_kinematics(ql_deg)
                    p_l = T_l[:3, 3]
                    rpy_l = _rotmat_to_rpy_zyx(T_l[:3, :3])
                    
                    # è®¡ç®—ç›¸å¯¹å˜åŒ–ï¼ˆdeltaï¼‰
                    delta_p_l = p_l - p_l_state
                    delta_rpy_l = rpy_l - rpy_l_state
                    
                    # gripperä½¿ç”¨ç»å¯¹å€¼ï¼ˆä¸æ˜¯deltaï¼‰
                    l_grip = float(joint_action[6])
                    
                    # å³è‡‚FK
                    qr_rad = joint_action[7:13].astype(float)
                    qr_deg = np.rad2deg(qr_rad)
                    T_r = right_kin.forward_kinematics(qr_deg)
                    p_r = T_r[:3, 3]
                    rpy_r = _rotmat_to_rpy_zyx(T_r[:3, :3])
                    
                    # è®¡ç®—ç›¸å¯¹å˜åŒ–ï¼ˆdeltaï¼‰
                    delta_p_r = p_r - p_r_state
                    delta_rpy_r = rpy_r - rpy_r_state
                    
                    # gripperä½¿ç”¨ç»å¯¹å€¼ï¼ˆä¸æ˜¯deltaï¼‰
                    r_grip = float(joint_action[13])
                    
                    # ç»„åˆï¼šendposeç”¨deltaï¼Œgripperç”¨ç»å¯¹å€¼
                    endpose = np.array([
                        delta_p_l[0], delta_p_l[1], delta_p_l[2], 
                        delta_rpy_l[0], delta_rpy_l[1], delta_rpy_l[2], 
                        l_grip,  # ç»å¯¹å€¼
                        delta_p_r[0], delta_p_r[1], delta_p_r[2], 
                        delta_rpy_r[0], delta_rpy_r[1], delta_rpy_r[2], 
                        r_grip,  # ç»å¯¹å€¼
                    ], dtype=np.float32)
                    gt_endpose_list.append(endpose)
                
                gt_endpose = torch.from_numpy(np.stack(gt_endpose_list, axis=0))  # [T, 14]
                
                # 2. æ¨¡å‹é¢„æµ‹å·²ç»æ˜¯endposeï¼ˆå› ä¸ºuse_endpose=Trueï¼‰
                pred_endpose = pred_full[0, :T, :14]  # [T, 14]
                
                # 3. å¯¹æ¯”endposeè¯¯å·®ï¼ˆåªçœ‹å·¦è‡‚å‰7ç»´ï¼‰
                pred_endpose_left = pred_endpose[:, :7].numpy()
                gt_endpose_left = gt_endpose[:, :7].numpy()
                
                if chunk_id == 0:
                    print("\n[FK Test] Chunk 0 è¯¦ç»†å¯¹æ¯” (å‰3æ­¥):")
                    print("æ³¨æ„ï¼šGTæ˜¯FK(action)-FK(state)çš„delta endpose + ç»å¯¹gripper")
                    print("      æ¨¡å‹è¾“å‡ºæ˜¯delta endpose + ç»å¯¹gripper")
                    for t in range(min(3, T)):
                        print(f"\n  æ—¶é—´æ­¥ t={t}:")
                        print(f"    GT_joint_abs[{t}]:        {gt_full_joint[t, :7]}")  # åŸå§‹jointï¼ˆç»å¯¹å€¼ï¼‰
                        print(f"    GT_endpose_delta[{t}]:    {gt_endpose_left[t]}")    # FKè½¬æ¢åçš„delta endpose
                        print(f"    Pred_endpose_delta[{t}]:  {pred_endpose_left[t]}")  # æ¨¡å‹è¾“å‡ºçš„delta endpose
                        print(f"    diff:                     {np.abs(pred_endpose_left[t] - gt_endpose_left[t])}")
                        print(f"    ä½ç½®deltaè¯¯å·®(m):   {np.linalg.norm(pred_endpose_left[t, :3] - gt_endpose_left[t, :3]):.6f}")
                        print(f"    å§¿æ€deltaè¯¯å·®(rad): {np.linalg.norm(pred_endpose_left[t, 3:6] - gt_endpose_left[t, 3:6]):.6f}")
                        print(f"    å¤¹çˆªç»å¯¹å€¼è¯¯å·®:     {np.abs(pred_endpose_left[t, 6] - gt_endpose_left[t, 6]):.6f}")
                
                # è®¡ç®—æ•´ä½“endposeè¯¯å·®
                pos_error = np.linalg.norm(pred_endpose_left[:, :3] - gt_endpose_left[:, :3], axis=1)
                ori_error = np.linalg.norm(pred_endpose_left[:, 3:6] - gt_endpose_left[:, 3:6], axis=1)
                grip_error = np.abs(pred_endpose_left[:, 6] - gt_endpose_left[:, 6])
                
                print(f"\n[FK Test] Chunk {chunk_id} Endpose Delta + Gripper Absolute æ•´ä½“è¯¯å·®:")
                print(f"  ä½ç½®delta MAE: {pos_error.mean():.6f} m (max: {pos_error.max():.6f} m)")
                print(f"  å§¿æ€delta MAE: {ori_error.mean():.6f} rad (max: {ori_error.max():.6f} rad)")
                print(f"  å¤¹çˆªç»å¯¹å€¼ MAE: {grip_error.mean():.6f} (max: {grip_error.max():.6f})")
                
                # ä½¿ç”¨è½¬æ¢åçš„endposeä½œä¸ºGTè¿›è¡Œåç»­å¯¹æ¯”
                gt_full = gt_endpose
            else:
                # ä¸åšFKè½¬æ¢ï¼Œç›´æ¥ä½¿ç”¨jointä½œä¸ºGT
                gt_full = gt_full_joint
            # ===================== END FKè½¬æ¢æµ‹è¯• =====================
            
            # ğŸ”§ DEBUG: æ‰“å°ç¬¬ä¸€ä¸ªchunkçš„å‰3ä¸ªæ—¶é—´æ­¥çš„è¯¦ç»†ä¿¡æ¯ï¼ˆJointç©ºé—´ï¼‰
            if chunk_id == 0:
                print("\n[DEBUG] Chunk 0 Jointç©ºé—´è¯¦ç»†å¯¹æ¯”:")
                print(f"state0: {state0.cpu().squeeze()[:7]}")
                for t in range(min(3, T)):
                    print(f"\næ—¶é—´æ­¥ t={t}:")
                    print(f"  pred_action[{t}]: {pred_full[0, t, :7]}")
                    print(f"  gt_action[{t}]:   {gt_full[t, :7]}")
                    print(f"  diff[{t}]:        {(pred_full[0, t, :7] - gt_full[t, :7]).abs()}")
            
            # ===================== åªå¯¹æ¯”å·¦è‡‚ï¼ˆå‰7ç»´ï¼‰=====================
            pred = pred_full[0, :T, :eval_dim]                              # [T,7] æ¨¡å‹é¢„æµ‹ï¼ˆç»å¯¹å€¼ï¼‰
            gt = gt_full[:T, :eval_dim]                                     # [T,7] æ•°æ®é›†GTï¼ˆç»å¯¹å€¼ï¼‰
            

            # sanity
            if T == 0:
                print(f"\n[WARN] empty T at step={cur}. stop.")
                break

            segs = _segments(T)

            # compute per-chunk segment metrics
            print("\n" + "-" * 100)
            print(f"[CHUNK {chunk_id:03d}] step={cur:04d}  task={task}")
            print(f"         eval_T={T}  eval_dim={eval_dim} (å·¦è‡‚)")

            for name, sl in segs:
                p = pred[sl, :]
                g = gt[sl, :]
                if p.numel() == 0:
                    continue

                if len(joint_idx) > 0:
                    dj = (p[:, joint_idx] - g[:, joint_idx]).abs()
                    joint_mae = dj.mean().item()
                    joint_mse = (dj ** 2).mean().item()
                    # accumulate totals
                    ep_joint_abs_sum += dj.sum().item()
                    ep_joint_count += int(dj.numel())
                else:
                    joint_mae = float("nan")
                    joint_mse = float("nan")

                if len(gripper_idx) > 0:
                    dg = (p[:, gripper_idx] - g[:, gripper_idx]).abs()
                    grip_mae = dg.mean().item()
                    grip_mse = (dg ** 2).mean().item()
                    # accumulate totals
                    ep_grip_abs_sum += dg.sum().item()
                    ep_grip_count += int(dg.numel())
                else:
                    grip_mae = float("nan")
                    grip_mse = float("nan")

                print(
                    f"  [{name:>6}] "
                    f"joints: MAE={joint_mae:.6f} MSE={joint_mse:.6f}   "
                    f"gripper: MAE={grip_mae:.6f} MSE={grip_mse:.6f}"
                )


            # advance to next chunk (IMPORTANT: chunked dataset usage)
            chunk_id += 1
            cur += chunk_len

            if cur >= episode_length:
                print(f"\n[STOP] reached episode end: cur={cur} >= episode_length={episode_length}")
                break

        # episode summary
        ep_joint_mae = ep_joint_abs_sum / max(1, ep_joint_count)
        ep_grip_mae = ep_grip_abs_sum / max(1, ep_grip_count)

        print("\n" + "=" * 100)
        print(f"[SUMMARY] episode_idx={episode_idx}  tested_chunks={chunk_id}  tested_steps={chunk_id * chunk_len}")
        print(f"  joints  : overall MAE={ep_joint_mae:.6f}   (count={ep_joint_count})")
        print(f"  gripper : overall MAE={ep_grip_mae:.6f}   (count={ep_grip_count})")
        print("=" * 100 + "\n")
    # ===================== END CHANGED =====================


def test_pi0_with_lerobot_dataset(
    model_path: str,
    dataset_path: str,
    episode_idx: int = 0,
    start_step: int = 0,
    max_chunks: int | None = None,
    device: str = "cuda:7",
    test_fk_conversion: bool = False,
):
    """æµ‹è¯• PI0 æ¨¡å‹åœ¨ lerobot æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆchunked evaluationï¼‰ã€‚
    
    Args:
        test_fk_conversion: å¦‚æœä¸ºTrueï¼Œå°†joint actioné€šè¿‡FKè½¬æ¢ä¸ºendposeè¿›è¡Œå¯¹æ¯”æµ‹è¯•
    """
    logger.info("åˆå§‹åŒ–æ¨¡å‹...")

    from transformers import AutoTokenizer
    from recipe.vla.models.pi0_torch.modeling_pi0_torch import PI0ForActionPrediction
    from recipe.vla.models.pi0_torch.configuration_pi0_torch import PI0TorchConfig

    logger.info(f"ä» {model_path} åŠ è½½ PI0 æ¨¡å‹...")

    config = PI0TorchConfig.from_pretrained(model_path)
    if hasattr(config, "attn_implementation"):
        config.attn_implementation = "eager"

    model = PI0ForActionPrediction.from_pretrained(model_path, config=config)
    model = model.to(device)
    model.eval()

    logger.info(f"æ¨¡å‹é…ç½®: pi05_enabled={config.pi05_enabled}, use_endpose={getattr(config, 'use_endpose', False)}, no_state={getattr(config, 'no_state', False)}, dtype={model.dtype}")

    logger.info("åŠ è½½ tokenizer...")
    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=False)

    model_config = {"path": model_path}
    rollout = PI0RolloutRob(
        model_config=model_config,
        module=model,
        tokenizer=tokenizer,
    )
    rollout.enable_input_saving(base_path="/shared_disk/users/weijie.ke/verl/recipe/vla/obs")
    logger.info(f"åŠ è½½æ•°æ®é›†: {dataset_path}")
    rollout.load_test_dataset(dataset_path)

    rollout.test_episode_chunked(
        episode_idx=episode_idx,
        start_step=start_step,
        max_chunks=max_chunks,
        verbose=False,
        test_fk_conversion=test_fk_conversion,
    )

    logger.info("æµ‹è¯•å®Œæˆ!")


if __name__ == "__main__":
    import argparse

    DEFAULT_MODEL_PATH = "/shared_disk/users/weijie.ke/weight/giga-openpi/pick_catch_bowl_eepose"
    DEFAULT_DATASET_PATH = "/shared_disk/users/yejun.zeng/datasets/huggingface/lerobot/catch_bowl"
    DEFAULT_EPISODE_IDX = 0
    DEFAULT_START_STEP = 0
    DEFAULT_DEVICE = "cuda:4"

    parser = argparse.ArgumentParser(
        description="æµ‹è¯• PI0 æ¨¡å‹åœ¨ lerobot æ•°æ®é›†ä¸Šçš„è¡¨ç°ï¼ˆchunked, 30-stepï¼‰",
        formatter_class=argparse.ArgumentDefaultsHelpFormatter,
    )
    parser.add_argument("--model_path", type=str, default=DEFAULT_MODEL_PATH, help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--dataset_path", type=str, default=DEFAULT_DATASET_PATH, help="lerobot æ•°æ®é›†è·¯å¾„")
    parser.add_argument("--episode_idx", type=int, default=DEFAULT_EPISODE_IDX, help="episode ç´¢å¼•")
    parser.add_argument("--start_step", type=int, default=DEFAULT_START_STEP, help="èµ·å§‹æ­¥æ•°ï¼ˆchunkèµ·ç‚¹ï¼‰")
    parser.add_argument("--max_chunks", type=int, default=None, help="æœ€å¤šæµ‹è¯•å¤šå°‘ä¸ªchunkï¼ˆNone=è·‘åˆ°ä¸å¤Ÿ30æ­¥ä¸ºæ­¢ï¼‰")
    parser.add_argument("--device", type=str, default=DEFAULT_DEVICE, help="è®¾å¤‡")
    parser.add_argument("--test_fk", action="store_true", default=True, help="æ˜¯å¦æµ‹è¯•FKè½¬æ¢ï¼ˆå°†jointè½¬ä¸ºendposeå¯¹æ¯”ï¼‰ï¼Œé»˜è®¤å¯ç”¨")

    args = parser.parse_args()

    logger.info("=" * 80)
    logger.info("PI0 chunked æµ‹è¯•é…ç½®:")
    logger.info(f"  æ¨¡å‹è·¯å¾„: {args.model_path}")
    logger.info(f"  æ•°æ®é›†è·¯å¾„: {args.dataset_path}")
    logger.info(f"  Episode ç´¢å¼•: {args.episode_idx}")
    logger.info(f"  èµ·å§‹æ­¥æ•°: {args.start_step}")
    logger.info(f"  max_chunks: {args.max_chunks}")
    logger.info(f"  è®¾å¤‡: {args.device}")
    logger.info(f"  FKè½¬æ¢æµ‹è¯•: {args.test_fk}")
    logger.info("=" * 80)

    test_pi0_with_lerobot_dataset(
        model_path=args.model_path,
        dataset_path=args.dataset_path,
        episode_idx=args.episode_idx,
        start_step=args.start_step,
        max_chunks=args.max_chunks,
        device=args.device,
        test_fk_conversion=args.test_fk,
    )
